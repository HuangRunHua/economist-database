{"url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road", "__typename": "URL"}, "__typename": "Content", "id": "/content/hrqlfvna2tgljoejdrouegv1m6q6ncbb", "tegID": "hrqlfvna2tgljoejdrouegv1m6q6ncbb", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "The bigger-is-better approach to AI is running out of road", "subheadline": "Anything that can\u2019t continue, won\u2019t", "seoPageTitle": null, "seoMetadataDescription": null, "ad": {"grapeshot": {"channels": [{"name": "gv_safe", "score": 29088.342, "__typename": "GrapeshotChannel"}, {"name": "future_of_work_test", "score": 5.74, "__typename": "GrapeshotChannel"}, {"name": "gb_safe", "score": 4.718, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_computing", "score": 4.709, "__typename": "GrapeshotChannel"}, {"name": "ibm_cloud", "score": 3.017, "__typename": "GrapeshotChannel"}, {"name": "ts_tech", "score": 2.722, "__typename": "GrapeshotChannel"}, {"name": "gs_tech", "score": 2.468, "__typename": "GrapeshotChannel"}, {"name": "neg_facebook", "score": 2.352, "__typename": "GrapeshotChannel"}, {"name": "neg_google_youtube_2020", "score": 2.301, "__typename": "GrapeshotChannel"}, {"name": "ge_tech_enthusiasts", "score": 2.096, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_compute", "score": 2.01, "__typename": "GrapeshotChannel"}, {"name": "gt_positive", "score": 1.844, "__typename": "GrapeshotChannel"}, {"name": "artificial_intelligence", "score": 1.716, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin_business", "score": 1.658, "__typename": "GrapeshotChannel"}, {"name": "gs_education", "score": 1.498, "__typename": "GrapeshotChannel"}, {"name": "custom_punkt", "score": 1.415, "__typename": "GrapeshotChannel"}, {"name": "gs_science_misc", "score": 1.368, "__typename": "GrapeshotChannel"}, {"name": "it_decisionmakers", "score": 1.336, "__typename": "GrapeshotChannel"}, {"name": "neg_3166_vca_brand-safety", "score": 1.332, "__typename": "GrapeshotChannel"}, {"name": "america_department_commerce", "score": 1.235, "__typename": "GrapeshotChannel"}, {"name": "it_professionals", "score": 1.196, "__typename": "GrapeshotChannel"}, {"name": "america_department_education", "score": 1.187, "__typename": "GrapeshotChannel"}, {"name": "cigna_healthyhybridworkplace", "score": 1.179, "__typename": "GrapeshotChannel"}, {"name": "fow_barclays", "score": 1.177, "__typename": "GrapeshotChannel"}, {"name": "neg_omd_exclusion", "score": 1.094, "__typename": "GrapeshotChannel"}, {"name": "gt_positive_curiosity", "score": 1.051, "__typename": "GrapeshotChannel"}, {"name": "neg_3166_vca_brand-safety3", "score": 1.02, "__typename": "GrapeshotChannel"}, {"name": "test_mba", "score": 1.02, "__typename": "GrapeshotChannel"}, {"name": "business_it_decisionmakers", "score": 0.978, "__typename": "GrapeshotChannel"}, {"name": "ibm_blacklist", "score": 0.894, "__typename": "GrapeshotChannel"}, {"name": "fidelity_investment", "score": 0.894, "__typename": "GrapeshotChannel"}, {"name": "investment_banking_personal_finance", "score": 0.894, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_ai", "score": 0.88, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin", "score": 0.845, "__typename": "GrapeshotChannel"}, {"name": "ssga_sp400", "score": 0.786, "__typename": "GrapeshotChannel"}, {"name": "gs_education_misc", "score": 0.78, "__typename": "GrapeshotChannel"}], "__typename": "Grapeshot"}, "__typename": "Ad"}, "audio": {"main": null, "__typename": "Media"}, "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_STD001.jpg", "__typename": "URL"}, "__typename": "Content", "height": 720, "width": 1280, "description": ""}, "promo": null, "__typename": "Media"}, "description": "If AI is to keep getting better, it will have to do more with less", "datePublished": "2023-06-21T18:23:38Z", "dateModified": "2023-06-22T13:46:56Z", "dateRevised": "2023-06-21T18:23:38Z", "dateRevisedString": "Jun 21st 2023", "datePublishedString": "Jun 21st 2023", "dateCreated": "2023-06-21T10:20:00Z", "copyrightYear": 2023, "inLanguage": "en", "byline": "", "dateline": null, "text": [{"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "span", "attribs": {"data-caps": "initial"}, "children": [{"data": "W", "type": "text"}]}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "hen it", "type": "text"}]}, {"data": " ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "comes ", "type": "text"}]}, {"data": "to \u201clarge language models\u201d (", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s) such as ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "\u2014which powers Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": ", a popular chatbot made by Open", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ", an American research lab\u2014the clue is in the name. Modern ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems are powered by vast artificial neural networks, bits of software modelled, very loosely, on biological brains. ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-3, an ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": " released in 2020, was a behemoth. It had 175bn \u201cparameters\u201d, as the simulated connections between those neurons are called. It was trained by having thousands of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPU", "type": "text"}]}, {"data": "s (specialised chips that excel at ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " work) crunch through hundreds of billions of words of text over the course of several weeks. All that is thought to have cost at least $4.6m.", "type": "text"}]}, {"type": "tag", "name": "figure", "attribs": {"itemtype": "https://schema.org/ImageObject"}, "children": [{"type": "tag", "name": "img", "attribs": {"src": "https://www.economist.com/media-assets/image/20230624_STC930.png", "data-teg-id": "dhid7ejsqjc68doae5khi9fd5dppd9sp", "data-slim": "1", "height": "739", "width": "608"}, "children": []}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "But the most consistent result from modern ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " research is that, while big is good, bigger is better. Models have therefore been growing at a blistering pace. ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-4, released in March, is thought to have around 1trn parameters\u2014nearly six times as many as its predecessor. Sam Altman, the firm\u2019s boss, put its development costs at more than $100m. Similar trends exist across the industry. Epoch ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ", a research firm, estimated in 2022 that the computing power necessary to train a cutting-edge model was doubling every six to ten months (see chart). ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "This gigantism is becoming a problem. If Epoch ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "\u2019s ten-monthly doubling figure is right, then training costs could exceed a billion dollars by 2026\u2014assuming, that is, models do not run out of data first. An analysis published in October 2022 forecast that the stock of high-quality text for training may well be exhausted around the same time. And even once the training is complete, actually using the resulting model can be expensive as well. The bigger the model, the more it costs to run. Earlier this year Morgan Stanley, a bank, guessed that, were half of Google\u2019s searches to be handled by a current ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-style program, it could cost the firm an additional $6bn a year. As the models get bigger, that number will probably rise.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Many in the field therefore think the \u201cbigger is better\u201d approach is running out of road. If ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " models are to carry on improving\u2014never mind fulfilling the ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "-related dreams currently sweeping the tech industry\u2014their creators will need to work out how to get more performance out of fewer resources. As Mr Altman put it in April, reflecting on the history of giant-sized ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ": \u201cI think we\u2019re at the end of an era.\u201d ", "type": "text"}]}, {"type": "tag", "name": "h2", "attribs": {}, "children": [{"data": "Quantitative tightening", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Instead, researchers are beginning to turn their attention to making their models more efficient, rather than simply bigger. One approach is to make trade-offs, cutting the number of parameters but training models with more data. In 2022 researchers at DeepMind, a division of Google, trained Chinchilla, an ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": " with 70bn parameters, on a corpus of 1.4trn words. The model outperforms ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-3, which has 175bn parameters trained on 300bn words. Feeding a smaller ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": " more data means it takes longer to train. But the result is a smaller model that is faster and cheaper to use. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Another option is to make the maths fuzzier. Tracking fewer decimal places for each number in the model\u2014rounding them off, in other words\u2014can cut hardware requirements drastically. In March researchers at the Institute of Science and Technology in Austria showed that rounding could squash the amount of memory consumed by a model similar to ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-3, allowing the model to run on one high-end ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPU", "type": "text"}]}, {"data": " instead of five, and with only \u201cnegligible accuracy degradation\u201d. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Some users fine-tune general-purpose ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s to focus on a specific task such as generating legal documents or detecting fake news. That is not as cumbersome as training an ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": " in the first place, but can still be costly and slow. Fine-tuning ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LL", "type": "text"}]}, {"data": "a", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "MA", "type": "text"}]}, {"data": ", an open-source model with 65bn parameters that was built by Meta, Facebook\u2019s corporate parent, takes multiple ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPU", "type": "text"}]}, {"data": "s anywhere from several hours to a few days. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Researchers at the University of Washington have invented a more efficient method that allowed them to create a new model, Guanaco, from ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LL", "type": "text"}]}, {"data": "a", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "MA", "type": "text"}]}, {"data": " on a single ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPU", "type": "text"}]}, {"data": " in a day without sacrificing much, if any, performance. Part of the trick was to use a similar rounding technique to the Austrians. But they also used a technique called \u201clow-rank adaptation\u201d, which involves freezing a model\u2019s existing parameters, then adding a new, smaller set of parameters in between. The fine-tuning is done by altering only those new variables. This simplifies things enough that even relatively feeble computers such as smartphones might be up to the task. Allowing ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s to live on a user\u2019s device, rather than in the giant data centres they currently inhabit, could allow for both greater personalisation and more privacy. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "A team at Google, meanwhile, has come up with a different option for those who can get by with smaller models. This approach focuses on extracting the specific knowledge required from a big, general-purpose model into a smaller, specialised one. The big model acts as a teacher, and the smaller as a student. The researchers ask the teacher to answer questions and show how it comes to its conclusions. Both the answers and the teacher\u2019s reasoning are used to train the student model. The team was able to train a student model with just 770m parameters, which outperformed its 540bn-parameter teacher on a specialised reasoning task.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Rather than focus on what the models are doing, another approach is to change how they are made. A great deal of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " programming is done in a language called Python. It is designed to be easy to use, freeing coders from the need to think about exactly how their programs will behave on the chips that run them. The price of abstracting such details away is slow code. Paying more attention to these implementation details can bring big benefits. This is \u201ca huge part of the game at the moment\u201d, says Thomas Wolf, chief science officer of Hugging Face, an open-source ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " company. ", "type": "text"}]}, {"type": "tag", "name": "h2", "attribs": {}, "children": [{"data": "Learn to code", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "In 2022, for instance, researchers at Stanford University published a modified version of the \u201cattention algorithm\u201d, which allows ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s to learn connections between words and ideas. The idea was to modify the code to take account of what is happening on the chip that is running it, and especially to keep track of when a given piece of information needs to be looked up or stored. Their algorithm was able to speed up the training of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-2, an older large language model, threefold. It also gave it the ability to respond to longer queries. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Sleeker code can also come from better tools. Earlier this year, Meta released an updated version of PyTorch, an ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "ai", "type": "text"}]}, {"data": "-programming framework. By allowing coders to think more about how computations are arranged on the actual chip, it can double a model\u2019s training speed by adding just one line of code. Modular, a startup founded by former engineers at Apple and Google, last month released a new ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "-focused programming language called Mojo, which is based on Python. It too gives coders control over all sorts of fine details that were previously hidden. In some cases, code written in Mojo can run thousands of times faster than the same code in Python.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "A final option is to improve the chips on which that code runs. ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPU", "type": "text"}]}, {"data": "s are only accidentally good at running ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " software\u2014they were originally designed to process the fancy graphics in modern video games. In particular, says a hardware researcher at Meta, ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPU", "type": "text"}]}, {"data": "s are imperfectly designed for \u201cinference\u201d work (ie, actually running a model once it has been trained). Some firms are therefore designing their own, more specialised hardware. Google already runs most of its ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " projects on its in-house \u201c", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "TPU", "type": "text"}]}, {"data": "\u201d chips. Meta, with its ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "MTIA", "type": "text"}]}, {"data": "s, and Amazon, with its Inferentia chips, are pursuing a similar path. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "That such big performance increases can be extracted from relatively simple changes like rounding numbers or switching programming languages might seem surprising. But it reflects the breakneck speed with which ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s have been developed. For many years they were research projects, and simply getting them to work well was more important than making them elegant. Only recently have they graduated to commercial, mass-market products. Most experts think there remains plenty of room for improvement. As Chris Manning, a computer scientist at Stanford University, put it: \u201cThere\u2019s absolutely no reason to believe\u2026that this is the ultimate neural architecture, and we will never find anything better.\u201d ", "type": "text"}, {"type": "tag", "name": "span", "attribs": {"data-ornament": "ufinish"}, "children": [{"data": "\u25a0", "type": "text"}]}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "i", "attribs": {}, "children": [{"data": "Curious about the world? To enjoy our mind-expanding science coverage, sign up to ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/newsletters/simply-science"}, "children": [{"data": "Simply Science", "type": "text"}]}, {"data": ", our weekly subscriber-only newsletter.", "type": "text"}]}]}], "bodyText": "When it comes to \u201clarge language models\u201d (LLMs) such as GPT\u2014which powers ChatGPT, a popular chatbot made by OpenAI, an American research lab\u2014the clue is in the name. Modern AI systems are powered by vast artificial neural networks, bits of software modelled, very loosely, on biological brains. GPT-3, an LLM released in 2020, was a behemoth. It had 175bn \u201cparameters\u201d, as the simulated connections between those neurons are called. It was trained by having thousands of GPUs (specialised chips that excel at AI work) crunch through hundreds of billions of words of text over the course of several weeks. All that is thought to have cost at least $4.6m.\n\nBut the most consistent result from modern AI research is that, while big is good, bigger is better. Models have therefore been growing at a blistering pace. GPT-4, released in March, is thought to have around 1trn parameters\u2014nearly six times as many as its predecessor. Sam Altman, the firm\u2019s boss, put its development costs at more than $100m. Similar trends exist across the industry. Epoch AI, a research firm, estimated in 2022 that the computing power necessary to train a cutting-edge model was doubling every six to ten months (see chart). \nThis gigantism is becoming a problem. If Epoch AI\u2019s ten-monthly doubling figure is right, then training costs could exceed a billion dollars by 2026\u2014assuming, that is, models do not run out of data first. An analysis published in October 2022 forecast that the stock of high-quality text for training may well be exhausted around the same time. And even once the training is complete, actually using the resulting model can be expensive as well. The bigger the model, the more it costs to run. Earlier this year Morgan Stanley, a bank, guessed that, were half of Google\u2019s searches to be handled by a current GPT-style program, it could cost the firm an additional $6bn a year. As the models get bigger, that number will probably rise.\nMany in the field therefore think the \u201cbigger is better\u201d approach is running out of road. If AI models are to carry on improving\u2014never mind fulfilling the AI-related dreams currently sweeping the tech industry\u2014their creators will need to work out how to get more performance out of fewer resources. As Mr Altman put it in April, reflecting on the history of giant-sized AI: \u201cI think we\u2019re at the end of an era.\u201d \nQuantitative tightening\nInstead, researchers are beginning to turn their attention to making their models more efficient, rather than simply bigger. One approach is to make trade-offs, cutting the number of parameters but training models with more data. In 2022 researchers at DeepMind, a division of Google, trained Chinchilla, an LLM with 70bn parameters, on a corpus of 1.4trn words. The model outperforms GPT-3, which has 175bn parameters trained on 300bn words. Feeding a smaller LLM more data means it takes longer to train. But the result is a smaller model that is faster and cheaper to use. \nAnother option is to make the maths fuzzier. Tracking fewer decimal places for each number in the model\u2014rounding them off, in other words\u2014can cut hardware requirements drastically. In March researchers at the Institute of Science and Technology in Austria showed that rounding could squash the amount of memory consumed by a model similar to GPT-3, allowing the model to run on one high-end GPU instead of five, and with only \u201cnegligible accuracy degradation\u201d. \nSome users fine-tune general-purpose LLMs to focus on a specific task such as generating legal documents or detecting fake news. That is not as cumbersome as training an LLM in the first place, but can still be costly and slow. Fine-tuning LLaMA, an open-source model with 65bn parameters that was built by Meta, Facebook\u2019s corporate parent, takes multiple GPUs anywhere from several hours to a few days. \nResearchers at the University of Washington have invented a more efficient method that allowed them to create a new model, Guanaco, from LLaMA on a single GPU in a day without sacrificing much, if any, performance. Part of the trick was to use a similar rounding technique to the Austrians. But they also used a technique called \u201clow-rank adaptation\u201d, which involves freezing a model\u2019s existing parameters, then adding a new, smaller set of parameters in between. The fine-tuning is done by altering only those new variables. This simplifies things enough that even relatively feeble computers such as smartphones might be up to the task. Allowing LLMs to live on a user\u2019s device, rather than in the giant data centres they currently inhabit, could allow for both greater personalisation and more privacy. \nA team at Google, meanwhile, has come up with a different option for those who can get by with smaller models. This approach focuses on extracting the specific knowledge required from a big, general-purpose model into a smaller, specialised one. The big model acts as a teacher, and the smaller as a student. The researchers ask the teacher to answer questions and show how it comes to its conclusions. Both the answers and the teacher\u2019s reasoning are used to train the student model. The team was able to train a student model with just 770m parameters, which outperformed its 540bn-parameter teacher on a specialised reasoning task.\nRather than focus on what the models are doing, another approach is to change how they are made. A great deal of AI programming is done in a language called Python. It is designed to be easy to use, freeing coders from the need to think about exactly how their programs will behave on the chips that run them. The price of abstracting such details away is slow code. Paying more attention to these implementation details can bring big benefits. This is \u201ca huge part of the game at the moment\u201d, says Thomas Wolf, chief science officer of Hugging Face, an open-source AI company. \nLearn to code\nIn 2022, for instance, researchers at Stanford University published a modified version of the \u201cattention algorithm\u201d, which allows LLMs to learn connections between words and ideas. The idea was to modify the code to take account of what is happening on the chip that is running it, and especially to keep track of when a given piece of information needs to be looked up or stored. Their algorithm was able to speed up the training of GPT-2, an older large language model, threefold. It also gave it the ability to respond to longer queries. \nSleeker code can also come from better tools. Earlier this year, Meta released an updated version of PyTorch, an ai-programming framework. By allowing coders to think more about how computations are arranged on the actual chip, it can double a model\u2019s training speed by adding just one line of code. Modular, a startup founded by former engineers at Apple and Google, last month released a new AI-focused programming language called Mojo, which is based on Python. It too gives coders control over all sorts of fine details that were previously hidden. In some cases, code written in Mojo can run thousands of times faster than the same code in Python.\nA final option is to improve the chips on which that code runs. GPUs are only accidentally good at running AI software\u2014they were originally designed to process the fancy graphics in modern video games. In particular, says a hardware researcher at Meta, GPUs are imperfectly designed for \u201cinference\u201d work (ie, actually running a model once it has been trained). Some firms are therefore designing their own, more specialised hardware. Google already runs most of its AI projects on its in-house \u201cTPU\u201d chips. Meta, with its MTIAs, and Amazon, with its Inferentia chips, are pursuing a similar path. \nThat such big performance increases can be extracted from relatively simple changes like rounding numbers or switching programming languages might seem surprising. But it reflects the breakneck speed with which LLMs have been developed. For many years they were research projects, and simply getting them to work well was more important than making them elegant. Only recently have they graduated to commercial, mass-market products. Most experts think there remains plenty of room for improvement. As Chris Manning, a computer scientist at Stanford University, put it: \u201cThere\u2019s absolutely no reason to believe\u2026that this is the ultimate neural architecture, and we will never find anything better.\u201d \u25a0\nCurious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.", "about": {"public": null, "__typename": "Taxonomies"}, "print": {"headline": "Time for a diet", "section": {"url": {"canonical": "https://www.economist.com/science-and-technology/", "__typename": "URL"}, "__typename": "Content", "headline": "Science & technology"}, "__typename": "Print"}, "articleSection": {"public": null, "internal": [{"url": {"canonical": "https://www.economist.com/science-and-technology/", "__typename": "URL"}, "__typename": "Content", "id": "/content/0q9q05sn1pkml185ggkt9md42em318ff", "tegID": "0q9q05sn1pkml185ggkt9md42em318ff", "headline": "Science & technology", "hasPart": {"parts": [{"url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/21/sweden-wants-to-build-an-entire-city-from-wood", "__typename": "URL"}, "__typename": "Content", "id": "/content/nk7ear3rcno6ac9spj5qjqcrjen99nch", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Sweden wants to build an entire city from wood", "subheadline": "Growing business", "datePublished": "2023-06-21T18:24:16Z", "description": "Modern timber buildings can be cheap, green and fireproof", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_STP504.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road", "__typename": "URL"}, "__typename": "Content", "id": "/content/hrqlfvna2tgljoejdrouegv1m6q6ncbb", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "The bigger-is-better approach to AI is running out of road", "subheadline": "Anything that can\u2019t continue, won\u2019t", "datePublished": "2023-06-21T18:23:38Z", "description": "If AI is to keep getting better, it will have to do more with less", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_STD001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/20/study-drugs-make-healthy-people-worse-at-problem-solving-not-better", "__typename": "URL"}, "__typename": "Content", "id": "/content/u5r35oha37tl7g546oh121umem992jdr", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Study drugs make healthy people worse at problem-solving, not better", "subheadline": "Brains in a pill", "datePublished": "2023-06-20T17:17:14Z", "description": "Users try harder, but are less competent", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_STP502.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/14/the-idea-of-holobionts-represents-a-paradigm-shift-in-biology", "__typename": "URL"}, "__typename": "Content", "id": "/content/tmuqute9svrqvets15oappr8rubv1avp", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "The idea of \u201cholobionts\u201d represents a paradigm shift in biology", "subheadline": "You are legion", "datePublished": "2023-06-14T17:37:34Z", "description": "These meta-organisms are made up of animals, plants, and the microbiota that live on and inside them", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230617_STD001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/14/theres-more-than-one-way-to-spay-a-cat", "__typename": "URL"}, "__typename": "Content", "id": "/content/so0mpesv3sk1ms1soulelv3johv5i6e4", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "There\u2019s more than one way to spay a cat", "subheadline": "Don\u2019t have kittens", "datePublished": "2023-06-14T17:33:49Z", "description": "A new injection could be a cheap, simple solution to a big problem", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230617_STP001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}], "__typename": "HasPart"}}], "__typename": "Taxonomies"}, "publication": [{"url": {"canonical": "https://www.economist.com/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/qkabpkufhvjuecehsigdv80dgtrq8ll6", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_EU.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "Building Ukraine 2.0", "width": 1280, "height": 1684, "regionsAllowed": ["AD", "AL", "AM", "AX", "AZ", "BA", "BG", "BY", "CH", "CY", "CZ", "EE", "FO", "GE", "GI", "GL", "HR", "HU", "IS", "KG", "KZ", "LI", "LT", "LV", "MC", "MD", "ME", "MK", "MT", "NO", "PL", "RO", "RS", "RU", "SI", "SJ", "SK", "SM", "TJ", "TM", "TR", "UA", "UZ", "VA", "IT", "FR", "ES", "IE", "AT", "BE", "DK", "FI", "DE", "GR", "LU", "NL", "PT", "SE"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["UK", "GB", "GG", "IM", "JE"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_EU.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/eu/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/pn35enf2bqvnje18ro0so515mbt4dge4", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_EU.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "Building Ukraine 2.0", "width": 1280, "height": 1684, "regionsAllowed": ["AD", "AL", "AM", "AX", "AZ", "BA", "BG", "BY", "CH", "CY", "CZ", "EE", "FO", "GE", "GI", "GL", "HR", "HU", "IS", "KG", "KZ", "LI", "LT", "LV", "MC", "MD", "ME", "MK", "MT", "NO", "PL", "RO", "RS", "RU", "SI", "SJ", "SK", "SM", "TJ", "TM", "TR", "UA", "UZ", "VA", "IT", "FR", "ES", "IE", "AT", "BE", "DK", "FI", "DE", "GR", "LU", "NL", "PT", "SE"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_EU.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_EU.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/ap/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/g3e1udr5nt09idpldiebr0eih02ns56v", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/la/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/ihgokq4bbqrrbb9dba3r973i2ngeo05t", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/me/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/331k10rju8qbelhhalsgjs5h25q608u3", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/uk/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/ul92kg7c3mq3hnof33tih3nrmr041md0", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["UK", "GB", "GG", "IM", "JE"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/na/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/aa35bm9jvsjud28rni1vmftgau95r56e", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/a/printedition/2023-06-24", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: JUNE 24TH 2023", "description": "", "subheadline": "", "datePublished": "2023-06-24T00:00:00Z", "datePublishedString": "Jun 24th 2023", "id": "/content/s8ffsdm78avvrtieuvcn1m7nmgkgu9om", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "The trouble with sticky inflation", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI", "AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230624_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}], "channel": {"tegID": "j53t6hsedat4l7rkbb1le98u73262sh5", "__typename": "Content"}, "_metadata": {"articleId": "/content/hrqlfvna2tgljoejdrouegv1m6q6ncbb", "tegID": "hrqlfvna2tgljoejdrouegv1m6q6ncbb", "title": "Anything that can\u2019t continue, won\u2019t - The bigger-is-better approach to AI is running out of road | Science & technology | The Economist", "shareSnippet": "Anything that can\u2019t continue, won\u2019t \u2013 The bigger-is-better approach to AI is running out of road", "headline": "The bigger-is-better approach to AI is running out of road", "section": "Science & technology", "keywords": [], "author": ["The Economist"], "url": "https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road", "type": "Article", "articleBody": "When it comes to \u201clarge language models\u201d (LLMs) such as GPT\u2014which powers ChatGPT, a popular chatbot made by OpenAI, an American research lab\u2014the clue is in the name. Modern AI systems are powered by vast artificial neural networks, bits of software modelled, very loosely, on biological brains. GPT-3, an LLM released in 2020, was a behemoth. It had 175bn \u201cparameters\u201d, as the simulated connections between those neurons are called. It was trained by having thousands of GPUs (specialised chips that excel at AI work) crunch through hundreds of billions of words of text over the course of several weeks. All that is thought to have cost at least $4.6m.\n\nBut the most consistent result from modern AI research is that, while big is good, bigger is better. Models have therefore been growing at a blistering pace. GPT-4, released in March, is thought to have around 1trn parameters\u2014nearly six times as many as its predecessor. Sam Altman, the firm\u2019s boss, put its development costs at more than $100m. Similar trends exist across the industry. Epoch AI, a research firm, estimated in 2022 that the computing power necessary to train a cutting-edge model was doubling every six to ten months (see chart). \nThis gigantism is becoming a problem. If Epoch AI\u2019s ten-monthly doubling figure is right, then training costs could exceed a billion dollars by 2026\u2014assuming, that is, models do not run out of data first. An analysis published in October 2022 forecast that the stock of high-quality text for training may well be exhausted around the same time. And even once the training is complete, actually using the resulting model can be expensive as well. The bigger the model, the more it costs to run. Earlier this year Morgan Stanley, a bank, guessed that, were half of Google\u2019s searches to be handled by a current GPT-style program, it could cost the firm an additional $6bn a year. As the models get bigger, that number will probably rise.\nMany in the field therefore think the \u201cbigger is better\u201d approach is running out of road. If AI models are to carry on improving\u2014never mind fulfilling the AI-related dreams currently sweeping the tech industry\u2014their creators will need to work out how to get more performance out of fewer resources. As Mr Altman put it in April, reflecting on the history of giant-sized AI: \u201cI think we\u2019re at the end of an era.\u201d \nQuantitative tightening\nInstead, researchers are beginning to turn their attention to making their models more efficient, rather than simply bigger. One approach is to make trade-offs, cutting the number of parameters but training models with more data. In 2022 researchers at DeepMind, a division of Google, trained Chinchilla, an LLM with 70bn parameters, on a corpus of 1.4trn words. The model outperforms GPT-3, which has 175bn parameters trained on 300bn words. Feeding a smaller LLM more data means it takes longer to train. But the result is a smaller model that is faster and cheaper to use. \nAnother option is to make the maths fuzzier. Tracking fewer decimal places for each number in the model\u2014rounding them off, in other words\u2014can cut hardware requirements drastically. In March researchers at the Institute of Science and Technology in Austria showed that rounding could squash the amount of memory consumed by a model similar to GPT-3, allowing the model to run on one high-end GPU instead of five, and with only \u201cnegligible accuracy degradation\u201d. \nSome users fine-tune general-purpose LLMs to focus on a specific task such as generating legal documents or detecting fake news. That is not as cumbersome as training an LLM in the first place, but can still be costly and slow. Fine-tuning LLaMA, an open-source model with 65bn parameters that was built by Meta, Facebook\u2019s corporate parent, takes multiple GPUs anywhere from several hours to a few days. \nResearchers at the University of Washington have invented a more efficient method that allowed them to create a new model, Guanaco, from LLaMA on a single GPU in a day without sacrificing much, if any, performance. Part of the trick was to use a similar rounding technique to the Austrians. But they also used a technique called \u201clow-rank adaptation\u201d, which involves freezing a model\u2019s existing parameters, then adding a new, smaller set of parameters in between. The fine-tuning is done by altering only those new variables. This simplifies things enough that even relatively feeble computers such as smartphones might be up to the task. Allowing LLMs to live on a user\u2019s device, rather than in the giant data centres they currently inhabit, could allow for both greater personalisation and more privacy. \nA team at Google, meanwhile, has come up with a different option for those who can get by with smaller models. This approach focuses on extracting the specific knowledge required from a big, general-purpose model into a smaller, specialised one. The big model acts as a teacher, and the smaller as a student. The researchers ask the teacher to answer questions and show how it comes to its conclusions. Both the answers and the teacher\u2019s reasoning are used to train the student model. The team was able to train a student model with just 770m parameters, which outperformed its 540bn-parameter teacher on a specialised reasoning task.\nRather than focus on what the models are doing, another approach is to change how they are made. A great deal of AI programming is done in a language called Python. It is designed to be easy to use, freeing coders from the need to think about exactly how their programs will behave on the chips that run them. The price of abstracting such details away is slow code. Paying more attention to these implementation details can bring big benefits. This is \u201ca huge part of the game at the moment\u201d, says Thomas Wolf, chief science officer of Hugging Face, an open-source AI company. \nLearn to code\nIn 2022, for instance, researchers at Stanford University published a modified version of the \u201cattention algorithm\u201d, which allows LLMs to learn connections between words and ideas. The idea was to modify the code to take account of what is happening on the chip that is running it, and especially to keep track of when a given piece of information needs to be looked up or stored. Their algorithm was able to speed up the training of GPT-2, an older large language model, threefold. It also gave it the ability to respond to longer queries. \nSleeker code can also come from better tools. Earlier this year, Meta released an updated version of PyTorch, an ai-programming framework. By allowing coders to think more about how computations are arranged on the actual chip, it can double a model\u2019s training speed by adding just one line of code. Modular, a startup founded by former engineers at Apple and Google, last month released a new AI-focused programming language called Mojo, which is based on Python. It too gives coders control over all sorts of fine details that were previously hidden. In some cases, code written in Mojo can run thousands of times faster than the same code in Python.\nA final option is to improve the chips on which that code runs. GPUs are only accidentally good at running AI software\u2014they were originally designed to process the fancy graphics in modern video games. In particular, says a hardware researcher at Meta, GPUs are imperfectly designed for \u201cinference\u201d work (ie, actually running a model once it has been trained). Some firms are therefore designing their own, more specialised hardware. Google already runs most of its AI projects on its in-house \u201cTPU\u201d chips. Meta, with its MTIAs, and Amazon, with its Inferentia chips, are pursuing a similar path. \nThat such big performance increases can be extracted from relatively simple changes like rounding numbers or switching programming languages might seem surprising. But it reflects the breakneck speed with which LLMs have been developed. For many years they were research projects, and simply getting them to work well was more important than making them elegant. Only recently have they graduated to commercial, mass-market products. Most experts think there remains plenty of room for improvement. As Chris Manning, a computer scientist at Stanford University, put it: \u201cThere\u2019s absolutely no reason to believe\u2026that this is the ultimate neural architecture, and we will never find anything better.\u201d \u25a0\nCurious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter.", "description": "If AI is to keep getting better, it will have to do more with less", "ogDescription": "If AI is to keep getting better, it will have to do more with less", "imageUrl": "https://www.economist.com/media-assets/image/20230624_STD001.jpg", "imageHeight": 720, "imageWidth": 1280, "datePublished": "2023-06-21T18:23:38Z", "dateModified": "2023-06-22T13:46:56Z", "dateCreated": "2023-06-21T10:20:00Z", "isPrintArticle": true, "printEdition": "2023-06-24T00:00:00Z", "copyrightYear": 2023, "dateline": "", "inLanguage": "en", "interactive": false, "scripts": [], "css": []}, "sectionArticles": [{"id": "/content/nk7ear3rcno6ac9spj5qjqcrjen99nch", "publication": [{"id": "/content/qkabpkufhvjuecehsigdv80dgtrq8ll6", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/pn35enf2bqvnje18ro0so515mbt4dge4", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/g3e1udr5nt09idpldiebr0eih02ns56v", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/ihgokq4bbqrrbb9dba3r973i2ngeo05t", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/331k10rju8qbelhhalsgjs5h25q608u3", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/ul92kg7c3mq3hnof33tih3nrmr041md0", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/aa35bm9jvsjud28rni1vmftgau95r56e", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/s8ffsdm78avvrtieuvcn1m7nmgkgu9om", "context": {"position": 1700.64, "__typename": "Content"}, "__typename": "Content"}], "headline": "Sweden wants to build an entire city from wood", "url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/21/sweden-wants-to-build-an-entire-city-from-wood", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/hrqlfvna2tgljoejdrouegv1m6q6ncbb", "publication": [{"id": "/content/qkabpkufhvjuecehsigdv80dgtrq8ll6", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/pn35enf2bqvnje18ro0so515mbt4dge4", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/g3e1udr5nt09idpldiebr0eih02ns56v", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/ihgokq4bbqrrbb9dba3r973i2ngeo05t", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/331k10rju8qbelhhalsgjs5h25q608u3", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/ul92kg7c3mq3hnof33tih3nrmr041md0", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/aa35bm9jvsjud28rni1vmftgau95r56e", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/s8ffsdm78avvrtieuvcn1m7nmgkgu9om", "context": {"position": 1700.62, "__typename": "Content"}, "__typename": "Content"}], "headline": "The bigger-is-better approach to AI is running out of road", "url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/21/the-bigger-is-better-approach-to-ai-is-running-out-of-road", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/u5r35oha37tl7g546oh121umem992jdr", "publication": [{"id": "/content/qkabpkufhvjuecehsigdv80dgtrq8ll6", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/pn35enf2bqvnje18ro0so515mbt4dge4", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/g3e1udr5nt09idpldiebr0eih02ns56v", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/ihgokq4bbqrrbb9dba3r973i2ngeo05t", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/331k10rju8qbelhhalsgjs5h25q608u3", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/ul92kg7c3mq3hnof33tih3nrmr041md0", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/aa35bm9jvsjud28rni1vmftgau95r56e", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/s8ffsdm78avvrtieuvcn1m7nmgkgu9om", "context": {"position": 1700.63, "__typename": "Content"}, "__typename": "Content"}], "headline": "Study drugs make healthy people worse at problem-solving, not better", "url": {"canonical": "https://www.economist.com/science-and-technology/2023/06/20/study-drugs-make-healthy-people-worse-at-problem-solving-not-better", "__typename": "URL"}, "__typename": "Content"}]}