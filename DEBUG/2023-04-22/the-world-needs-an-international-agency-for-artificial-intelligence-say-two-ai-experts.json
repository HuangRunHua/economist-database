{"url": {"canonical": "https://www.economist.com/by-invitation/2023/04/18/the-world-needs-an-international-agency-for-artificial-intelligence-say-two-ai-experts", "__typename": "URL"}, "__typename": "Content", "id": "/content/97rviamdk8sorcopa3sr5botkn06oo7d", "tegID": "97rviamdk8sorcopa3sr5botkn06oo7d", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "The world needs an international agency for artificial intelligence, say two AI experts", "subheadline": "Artificial intelligence", "seoPageTitle": null, "seoMetadataDescription": null, "ad": {"grapeshot": {"channels": [{"name": "gv_safe", "score": 23468.25, "__typename": "GrapeshotChannel"}, {"name": "future_of_work_test", "score": 4.457, "__typename": "GrapeshotChannel"}, {"name": "gs_tech", "score": 3.55, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_computing", "score": 3.122, "__typename": "GrapeshotChannel"}, {"name": "chanel_neg", "score": 2.805, "__typename": "GrapeshotChannel"}, {"name": "gt_negative", "score": 2.706, "__typename": "GrapeshotChannel"}, {"name": "ibm_cloud", "score": 2.375, "__typename": "GrapeshotChannel"}, {"name": "gs_politics", "score": 2.276, "__typename": "GrapeshotChannel"}, {"name": "neg_omd_exclusion", "score": 2.013, "__typename": "GrapeshotChannel"}, {"name": "neg_morgan_stanley_2019_neg_keywords", "score": 2.011, "__typename": "GrapeshotChannel"}, {"name": "test", "score": 1.995, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_ai", "score": 1.971, "__typename": "GrapeshotChannel"}, {"name": "neg_ey_kwbl", "score": 1.884, "__typename": "GrapeshotChannel"}, {"name": "neg_3166_vca_brand-safety", "score": 1.857, "__typename": "GrapeshotChannel"}, {"name": "neg_3166_vca_brand-safety3", "score": 1.857, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin", "score": 1.743, "__typename": "GrapeshotChannel"}, {"name": "neg_facebook", "score": 1.737, "__typename": "GrapeshotChannel"}, {"name": "ibm_blacklist", "score": 1.721, "__typename": "GrapeshotChannel"}, {"name": "artificial_intelligence", "score": 1.692, "__typename": "GrapeshotChannel"}, {"name": "neg_ey_brandsafety", "score": 1.594, "__typename": "GrapeshotChannel"}, {"name": "cigna_healthyhybridworkplace", "score": 1.584, "__typename": "GrapeshotChannel"}, {"name": "asia_nec_technology", "score": 1.553, "__typename": "GrapeshotChannel"}, {"name": "gs_science_misc", "score": 1.509, "__typename": "GrapeshotChannel"}, {"name": "ts_tech", "score": 1.5, "__typename": "GrapeshotChannel"}, {"name": "fow_barclays", "score": 1.476, "__typename": "GrapeshotChannel"}, {"name": "america_nanny_state", "score": 1.433, "__typename": "GrapeshotChannel"}, {"name": "google_negative_keywords", "score": 1.418, "__typename": "GrapeshotChannel"}, {"name": "custom_punkt", "score": 1.412, "__typename": "GrapeshotChannel"}, {"name": "legal_law", "score": 1.412, "__typename": "GrapeshotChannel"}, {"name": "neg_ibm_brandsafety", "score": 1.351, "__typename": "GrapeshotChannel"}, {"name": "business_it_decisionmakers", "score": 1.318, "__typename": "GrapeshotChannel"}, {"name": "cigna_puttingfamilyfirst", "score": 1.259, "__typename": "GrapeshotChannel"}, {"name": "future_of_work", "score": 1.241, "__typename": "GrapeshotChannel"}, {"name": "neg_google_youtube_2020", "score": 1.22, "__typename": "GrapeshotChannel"}, {"name": "test_mba", "score": 1.201, "__typename": "GrapeshotChannel"}, {"name": "gs_law", "score": 1.198, "__typename": "GrapeshotChannel"}, {"name": "america_department_security", "score": 1.179, "__typename": "GrapeshotChannel"}, {"name": "neg_huawei_brandsafety", "score": 1.171, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_robotics", "score": 1.146, "__typename": "GrapeshotChannel"}, {"name": "gs_education", "score": 1.117, "__typename": "GrapeshotChannel"}, {"name": "fidelity_blacklist", "score": 1.113, "__typename": "GrapeshotChannel"}, {"name": "america_campaigns_elections", "score": 1.094, "__typename": "GrapeshotChannel"}, {"name": "gs_business", "score": 1.093, "__typename": "GrapeshotChannel"}, {"name": "gt_negative_fear", "score": 1.028, "__typename": "GrapeshotChannel"}, {"name": "gs_politics_issues_policy", "score": 1.025, "__typename": "GrapeshotChannel"}, {"name": "america_department_education", "score": 1.001, "__typename": "GrapeshotChannel"}, {"name": "thought_leader", "score": 0.989, "__typename": "GrapeshotChannel"}, {"name": "neg_cartier", "score": 0.941, "__typename": "GrapeshotChannel"}, {"name": "america_department_commerce", "score": 0.923, "__typename": "GrapeshotChannel"}, {"name": "ge_tech_enthusiasts", "score": 0.921, "__typename": "GrapeshotChannel"}, {"name": "gt_negative_mistrust", "score": 0.919, "__typename": "GrapeshotChannel"}, {"name": "america_supreme_court", "score": 0.863, "__typename": "GrapeshotChannel"}, {"name": "gs_politics_misc", "score": 0.862, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin_indus", "score": 0.856, "__typename": "GrapeshotChannel"}, {"name": "gt_positive_curiosity", "score": 0.841, "__typename": "GrapeshotChannel"}, {"name": "hsbc_sustainability", "score": 0.828, "__typename": "GrapeshotChannel"}, {"name": "neg_mediakitchen_gs", "score": 0.822, "__typename": "GrapeshotChannel"}, {"name": "america_department_justice", "score": 0.817, "__typename": "GrapeshotChannel"}, {"name": "neg_exxon", "score": 0.812, "__typename": "GrapeshotChannel"}, {"name": "neg_dit9", "score": 0.769, "__typename": "GrapeshotChannel"}, {"name": "gs_politics_elections", "score": 0.764, "__typename": "GrapeshotChannel"}, {"name": "microsoft_blacklist", "score": 0.762, "__typename": "GrapeshotChannel"}, {"name": "neg_ey", "score": 0.753, "__typename": "GrapeshotChannel"}, {"name": "neg_dit", "score": 0.748, "__typename": "GrapeshotChannel"}, {"name": "it_decisionmakers", "score": 0.73, "__typename": "GrapeshotChannel"}], "__typename": "Grapeshot"}, "__typename": "Ad"}, "audio": {"main": null, "__typename": "Media"}, "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230418_BID001.jpg", "__typename": "URL"}, "__typename": "Content", "height": 720, "width": 1280, "description": ""}, "promo": null, "__typename": "Media"}, "description": "Gary Marcus and Anka Reuel argue that global governance must be prioritised to address the risks of bias, misinformation or worse", "datePublished": "2023-04-18T20:20:24Z", "dateModified": "2023-04-20T16:59:35Z", "dateModifiedString": "Apr 20th 2023", "datePublishedString": "Apr 18th 2023", "dateCreated": "2023-04-18T20:20:12Z", "copyrightYear": 2023, "inLanguage": "en", "byline": "", "dateline": null, "text": [{"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "span", "attribs": {"data-caps": "initial"}, "children": [{"data": "N", "type": "text"}]}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "EW GENERATIVE-AI", "type": "text"}]}, {"data": " tools like Open", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "\u2019s Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": ", the fastest-growing consumer internet application of all time, have taken the world by storm. They have uses in everything from education to medicine and are astonishingly fun to play with. Although current", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": " AI", "type": "text"}]}, {"data": " systems are capable of spectacular feats they also carry risks. Europol has warned that they might greatly increase cybercrime. Many ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " experts are deeply worried about their potential to create a tsunami of misinformation, posing an imminent threat to the American presidential election in 2024, and ultimately to democracy itself, by creating an atmosphere of total distrust. Scientists have warned that these new tools could be used to design novel, deadly toxins. Others speculate that in the long term there could be a genuine ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report_2023.pdf"}, "children": [{"data": "risk to humanity itself", "type": "text"}]}, {"data": ". ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "One of the key issues with current ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems is that they are primarily black boxes, often unreliable and hard to interpret, and at risk of getting out of control. For example, the core technology underlying systems like Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": ", large language models (", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLMs", "type": "text"}]}, {"data": "), is known to \u201challucinate\u201d, making up false statements. Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": ", for example, falsely accused a law professor of being involved in sexual harassment, apparently confused by statistical but irrelevant connections between bits of text that didn\u2019t actually belong together. After an op-ed tried to clarify what had gone wrong, Bing Chat made a similar error, and attributed it to information in ", "type": "text"}, {"type": "tag", "name": "i", "attribs": {}, "children": [{"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "USA", "type": "text"}]}, {"data": " Today", "type": "text"}]}, {"data": " that the chatbot got completely backwards. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "These systems can also be used for deliberate abuse, from disrupting elections (for example by manipulating what candidates appear to say or write) to spreading medical misinformation. In a recent analysis of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "-4, Open", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "\u2019s most advanced", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": " LLM", "type": "text"}]}, {"data": ", the company acknowledged 12 serious concerns\u2014without providing firm solutions to any of them. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "In the past year alone 37 regulations mentioning ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI ", "type": "text"}]}, {"data": "were passed around the globe; Italy went so far as to ban Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT.", "type": "text"}]}, {"data": " But there is little global co-ordination. Even within some countries there is a hodge-podge, such as different state laws in America, or Britain\u2019s proposal to eschew a central regulator, leaving oversight split among several agencies. An uneven, loophole-ridden patchwork is to no one\u2019s benefit and safety. Nor should companies want to build a different ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI ", "type": "text"}]}, {"data": "model for each jurisdiction and face their own de novo struggle to navigate legal, cultural and social contexts. ", "type": "text"}]}, {"type": "tag", "name": "aside", "attribs": {}, "children": [{"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Read more from our special series on ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ":", "type": "text"}]}, {"type": "tag", "name": "ul", "attribs": {}, "children": [{"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/interactive/science-and-technology/2023/04/22/large-creative-ai-models-will-transform-how-we-live-and-work", "data-tegid": "ad0pj0210lp3qhpd542inu4ti4vfs27o"}, "children": [{"data": "Large, creative ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " models will transform lives and labour markets", "type": "text"}]}]}, {"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/science-and-technology/2023/04/19/how-generative-models-could-go-wrong"}, "children": [{"data": "How generative models could go wrong", "type": "text"}]}]}, {"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/science-and-technology/2023/04/19/large-language-models-ability-to-generate-text-also-lets-them-plan-and-reason"}, "children": [{"data": "Large language models\u2019 ability to generate text also lets them plan and reason", "type": "text"}]}]}]}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Still, there is plenty of agreement about basic responsible ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " principles, such as safety and reliability, transparency, explainability, interpretability, privacy, accountability and fairness. And almost everyone agrees that something must be done\u2014a ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.governance.ai/post/increasing-consensus-ai-requires-careful-management"}, "children": [{"data": "just-published poll", "type": "text"}]}, {"data": " by the Centre for the Governance of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " found that 91% of a representative sample of 13,000 people across 11 countries agreed that", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": " AI", "type": "text"}]}, {"data": " needs to be carefully managed. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "It is in this context that we call for the immediate development of a global, neutral, non-profit International Agency for", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": " AI", "type": "text"}]}, {"data": " (", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "IAAI", "type": "text"}]}, {"data": "), with guidance and buy-in from governments, large technology companies, non-profits, academia and society at large, aimed at collaboratively finding governance and technical solutions to promote safe, secure and peaceful ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI ", "type": "text"}]}, {"data": "technologies. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The time for such an agency has come, as Google ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "CEO", "type": "text"}]}, {"data": " Sundar Pichai himself said on April 16th. What might that look like? Each domain and each industry will be different, with its own set of guidelines, but many will involve both global governance and technological innovation. For example, people have long agreed that making employment decisions based on gender should be avoided, and have even come up with some measures in earlier, more interpretable ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ", such as the interpretability requirements of the ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " Bill of Rights proposed by the Biden administration. But in black-box systems like Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": " there is a wide variety of use cases with no current remedy. People might, for example, feed in a job candidate\u2019s entire file and ask Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": " for a judgment, but we currently have no way to ensure that Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": " would avoid bias in its output. The kind of entity we envision would collaboratively address what to do about such \u201coff-label\u201d uses of chatbots and other policy questions, and at the same time develop technical tools for effective auditing. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "IAAI", "type": "text"}]}, {"data": " could likewise convene experts and develop tools to tackle the spread of misinformation. On the policy side, it could ask, for instance, how wide-scale spreading of misinformation might be penalised. On the technical side, the initial focus should be on developing automated or semi-automated tools for answering fundamental questions, such as \u201cHow much misinformation is out there?\u201d, \u201cHow rapidly is its volume growing?\u201d and \u201cHow much is ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " contributing to such problems?\u201d Existing technologies are better at generating misinformation than detecting it. Considerable technical innovation will be required, and of great public benefit, but may or may not be of sufficiently direct commercial interest \u2013 hence the need for independent support by an entity like the ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "IAAI", "type": "text"}]}, {"data": ". ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "To take a third, very recent example, systems with names like Auto", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": " and Baby", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AGI", "type": "text"}]}, {"data": " have been devised that allow amateurs to build complex and difficult-to-debug (or even fathom) assemblies of unreliable ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems controlling other unreliable ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems to achieve arbitrary goals\u2014a practice that may or may not prove to be safe. As Marek Rosa, ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "CEO", "type": "text"}]}, {"data": " of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GOOD", "type": "text"}]}, {"data": ".Ai, put it, we need new technical ideas on \u201chow to increase security (proactive defence) in a world where there are billions of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI ", "type": "text"}]}, {"data": "agents\u2026running in apps and servers, and we don\u2019t know what they are talking about\u201d, perhaps necessitating a kind of \u201cantivirus [software] against ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " agents\u201d. A global alliance with top experts and researchers on call would be able to give swift and thoughtful guidance on such new developments. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Designing the kind of global collaboration we envision is an enormous job. Many stakeholders need to be involved. Both short-term and long-term risks must be considered. No solution is going to succeed unless both governments and companies are on board, and it\u2019s not just them: the world\u2019s publics need a seat at the table. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Fortunately, there is precedent for such global co-operation. At the end of the second world war, for example, nuclear weapons sparked deep fears and uncertainties about how the new technology would be used. As a response, 81 countries unanimously approved the International Atomic Energy Agency\u2019s statute to \u201cpromote safe, secure and peaceful nuclear technologies\u201d, with inspection rights. A different, softer kind of model, with less focus on enforcement, is the International Civil Aviation Organisation, in which member countries make their own laws but take counsel from a global agency. Getting to the right model, and making the right choices, will take time, wisdom and collaboration. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The challenges and risks of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " are, of course, very different and, to a disconcerting degree, still unknown. We know in hindsight that the internet might have been designed in better ways with more forethought. Earlier decisions about how to handle privacy and anonymity, for instance, might have ensured that there was less of a culture of trolling. We also know that early choices get locked in. Our decisions now are likely to have lasting consequences and must be made thoughtfully.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Given how fast things are moving, there is not a lot of time to waste. A global, neutral non-profit with support from governments, big business and society is an important start. ", "type": "text"}, {"type": "tag", "name": "span", "attribs": {"data-ornament": "ufinish"}, "children": [{"data": "\u25a0", "type": "text"}]}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "i", "attribs": {}, "children": [{"data": "Gary Marcus is Emeritus Professor at NYU and was founder and ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "CEO ", "type": "text"}]}, {"data": "of Geometric Intelligence, a machine-learning company acquired by Uber. Anka Reuel is a PhD student in computer science at Stanford University and founding member of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "KIRA", "type": "text"}]}, {"data": ", a think-tank focusing on the promotion of responsible ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI.", "type": "text"}]}]}]}], "bodyText": "NEW GENERATIVE-AI tools like OpenAI\u2019s ChatGPT, the fastest-growing consumer internet application of all time, have taken the world by storm. They have uses in everything from education to medicine and are astonishingly fun to play with. Although current AI systems are capable of spectacular feats they also carry risks. Europol has warned that they might greatly increase cybercrime. Many AI experts are deeply worried about their potential to create a tsunami of misinformation, posing an imminent threat to the American presidential election in 2024, and ultimately to democracy itself, by creating an atmosphere of total distrust. Scientists have warned that these new tools could be used to design novel, deadly toxins. Others speculate that in the long term there could be a genuine risk to humanity itself. \nOne of the key issues with current AI systems is that they are primarily black boxes, often unreliable and hard to interpret, and at risk of getting out of control. For example, the core technology underlying systems like ChatGPT, large language models (LLMs), is known to \u201challucinate\u201d, making up false statements. ChatGPT, for example, falsely accused a law professor of being involved in sexual harassment, apparently confused by statistical but irrelevant connections between bits of text that didn\u2019t actually belong together. After an op-ed tried to clarify what had gone wrong, Bing Chat made a similar error, and attributed it to information in USA Today that the chatbot got completely backwards. \nThese systems can also be used for deliberate abuse, from disrupting elections (for example by manipulating what candidates appear to say or write) to spreading medical misinformation. In a recent analysis of GPT-4, OpenAI\u2019s most advanced LLM, the company acknowledged 12 serious concerns\u2014without providing firm solutions to any of them. \nIn the past year alone 37 regulations mentioning AI were passed around the globe; Italy went so far as to ban ChatGPT. But there is little global co-ordination. Even within some countries there is a hodge-podge, such as different state laws in America, or Britain\u2019s proposal to eschew a central regulator, leaving oversight split among several agencies. An uneven, loophole-ridden patchwork is to no one\u2019s benefit and safety. Nor should companies want to build a different AI model for each jurisdiction and face their own de novo struggle to navigate legal, cultural and social contexts. \nRead more from our special series on AI:\nLarge, creative AI models will transform lives and labour markets\nHow generative models could go wrong\nLarge language models\u2019 ability to generate text also lets them plan and reason\n\n\nStill, there is plenty of agreement about basic responsible AI principles, such as safety and reliability, transparency, explainability, interpretability, privacy, accountability and fairness. And almost everyone agrees that something must be done\u2014a just-published poll by the Centre for the Governance of AI found that 91% of a representative sample of 13,000 people across 11 countries agreed that AI needs to be carefully managed. \nIt is in this context that we call for the immediate development of a global, neutral, non-profit International Agency for AI (IAAI), with guidance and buy-in from governments, large technology companies, non-profits, academia and society at large, aimed at collaboratively finding governance and technical solutions to promote safe, secure and peaceful AI technologies. \nThe time for such an agency has come, as Google CEO Sundar Pichai himself said on April 16th. What might that look like? Each domain and each industry will be different, with its own set of guidelines, but many will involve both global governance and technological innovation. For example, people have long agreed that making employment decisions based on gender should be avoided, and have even come up with some measures in earlier, more interpretable AI, such as the interpretability requirements of the AI Bill of Rights proposed by the Biden administration. But in black-box systems like ChatGPT there is a wide variety of use cases with no current remedy. People might, for example, feed in a job candidate\u2019s entire file and ask ChatGPT for a judgment, but we currently have no way to ensure that ChatGPT would avoid bias in its output. The kind of entity we envision would collaboratively address what to do about such \u201coff-label\u201d uses of chatbots and other policy questions, and at the same time develop technical tools for effective auditing. \nThe IAAI could likewise convene experts and develop tools to tackle the spread of misinformation. On the policy side, it could ask, for instance, how wide-scale spreading of misinformation might be penalised. On the technical side, the initial focus should be on developing automated or semi-automated tools for answering fundamental questions, such as \u201cHow much misinformation is out there?\u201d, \u201cHow rapidly is its volume growing?\u201d and \u201cHow much is AI contributing to such problems?\u201d Existing technologies are better at generating misinformation than detecting it. Considerable technical innovation will be required, and of great public benefit, but may or may not be of sufficiently direct commercial interest \u2013 hence the need for independent support by an entity like the IAAI. \nTo take a third, very recent example, systems with names like AutoGPT and BabyAGI have been devised that allow amateurs to build complex and difficult-to-debug (or even fathom) assemblies of unreliable AI systems controlling other unreliable AI systems to achieve arbitrary goals\u2014a practice that may or may not prove to be safe. As Marek Rosa, CEO of GOOD.Ai, put it, we need new technical ideas on \u201chow to increase security (proactive defence) in a world where there are billions of AI agents\u2026running in apps and servers, and we don\u2019t know what they are talking about\u201d, perhaps necessitating a kind of \u201cantivirus [software] against AI agents\u201d. A global alliance with top experts and researchers on call would be able to give swift and thoughtful guidance on such new developments. \nDesigning the kind of global collaboration we envision is an enormous job. Many stakeholders need to be involved. Both short-term and long-term risks must be considered. No solution is going to succeed unless both governments and companies are on board, and it\u2019s not just them: the world\u2019s publics need a seat at the table. \nFortunately, there is precedent for such global co-operation. At the end of the second world war, for example, nuclear weapons sparked deep fears and uncertainties about how the new technology would be used. As a response, 81 countries unanimously approved the International Atomic Energy Agency\u2019s statute to \u201cpromote safe, secure and peaceful nuclear technologies\u201d, with inspection rights. A different, softer kind of model, with less focus on enforcement, is the International Civil Aviation Organisation, in which member countries make their own laws but take counsel from a global agency. Getting to the right model, and making the right choices, will take time, wisdom and collaboration. \nThe challenges and risks of AI are, of course, very different and, to a disconcerting degree, still unknown. We know in hindsight that the internet might have been designed in better ways with more forethought. Earlier decisions about how to handle privacy and anonymity, for instance, might have ensured that there was less of a culture of trolling. We also know that early choices get locked in. Our decisions now are likely to have lasting consequences and must be made thoughtfully.\nGiven how fast things are moving, there is not a lot of time to waste. A global, neutral non-profit with support from governments, big business and society is an important start. \u25a0\nGary Marcus is Emeritus Professor at NYU and was founder and CEO of Geometric Intelligence, a machine-learning company acquired by Uber. Anka Reuel is a PhD student in computer science at Stanford University and founding member of KIRA, a think-tank focusing on the promotion of responsible AI.", "about": {"public": null, "__typename": "Taxonomies"}, "print": {"headline": "The world needs an international agency for artificial intelligence, say two AI experts", "section": {"url": {"canonical": "https://www.economist.com/by-invitation/", "__typename": "URL"}, "__typename": "Content", "headline": "By Invitation"}, "__typename": "Print"}, "articleSection": {"public": null, "internal": [{"url": {"canonical": "https://www.economist.com/by-invitation/", "__typename": "URL"}, "__typename": "Content", "id": "/content/arf6v6apbfv7pa17gm9ipjqoulhgsg79", "tegID": "arf6v6apbfv7pa17gm9ipjqoulhgsg79", "headline": "By Invitation", "hasPart": {"parts": [{"url": {"canonical": "https://www.economist.com/by-invitation/2023/04/19/only-a-sustained-fall-in-oil-prices-will-break-vladimir-putins-regime-argues-kirill-rogov", "__typename": "URL"}, "__typename": "Content", "id": "/content/bf7p309sks2ro7g6rp29g80boshuln7t", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Only a sustained fall in oil prices will break Vladimir Putin\u2019s regime, argues Kirill Rogov", "subheadline": "Putin\u2019s Achilles heel", "datePublished": "2023-04-19T13:49:52Z", "description": "The political scientist says that sanctions have had a big economic impact, but energy revenues have dulled the pain", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20221008_BID001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/by-invitation/2023/04/18/the-world-needs-an-international-agency-for-artificial-intelligence-say-two-ai-experts", "__typename": "URL"}, "__typename": "Content", "id": "/content/97rviamdk8sorcopa3sr5botkn06oo7d", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "The world needs an international agency for artificial intelligence, say two AI experts", "subheadline": "Artificial intelligence", "datePublished": "2023-04-18T20:20:24Z", "description": "Gary Marcus and Anka Reuel argue that global governance must be prioritised to address the risks of bias, misinformation or worse", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230418_BID001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/by-invitation/2023/04/17/kaja-kallas-says-ukraine-is-giving-the-free-world-a-masterclass-on-cyber-defence", "__typename": "URL"}, "__typename": "Content", "id": "/content/o2tq002mahu14nbvv3vok4es8vjgnrs7", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Kaja Kallas says Ukraine is giving the free world a masterclass on cyber-defence", "subheadline": "Digital warfare", "datePublished": "2023-04-17T14:24:19Z", "description": "The prime minister of Estonia on the need for better preparation against digital warfare", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230415_BID001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/by-invitation/2023/04/13/intelligence-leaks-are-an-opportunity-as-well-as-a-threat-says-thomas-rid", "__typename": "URL"}, "__typename": "Content", "id": "/content/qaa79ilcs2ed5anjv1ndlb8f9qo2caap", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Intelligence leaks are an opportunity as well as a threat, says Thomas Rid", "subheadline": "The Pentagon leaks", "datePublished": "2023-04-13T13:08:46Z", "description": "The scholar of spycraft says the private sector learns a lot from security breaches", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230415_BID003.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/by-invitation/2023/04/12/jonathan-powell-on-preparations-for-peace-making-in-ukraine", "__typename": "URL"}, "__typename": "Content", "id": "/content/usclkoee8oauotgclq4agur5197evrsc", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Jonathan Powell on preparations for peace-making in Ukraine", "subheadline": "Russia and Ukraine", "datePublished": "2023-04-12T17:50:12Z", "description": "Tony Blair\u2019s former chief-of-staff says lessons from the Good Friday Agreement can help bring peace", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230415_BID002.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}], "__typename": "HasPart"}}], "__typename": "Taxonomies"}, "publication": [{"url": {"canonical": "https://www.economist.com/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_AP.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["UK", "GB", "GG", "IM", "JE", "AD", "AL", "AM", "AX", "AZ", "BA", "BG", "BY", "CH", "CY", "CZ", "EE", "FO", "GE", "GI", "GL", "HR", "HU", "IS", "KG", "KZ", "LI", "LT", "LV", "MC", "MD", "ME", "MK", "MT", "NO", "PL", "RO", "RS", "RU", "SI", "SJ", "SK", "SM", "TJ", "TM", "TR", "UA", "UZ", "VA", "IT", "FR", "ES", "IE", "AT", "BE", "DK", "FI", "DE", "GR", "LU", "NL", "PT", "SE", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_AP.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/eu/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/ap/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_AP.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_AP.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_AP.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/la/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/me/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/uk/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["UK", "GB", "GG", "IM", "JE", "AD", "AL", "AM", "AX", "AZ", "BA", "BG", "BY", "CH", "CY", "CZ", "EE", "FO", "GE", "GI", "GL", "HR", "HU", "IS", "KG", "KZ", "LI", "LT", "LV", "MC", "MD", "ME", "MK", "MT", "NO", "PL", "RO", "RS", "RU", "SI", "SJ", "SK", "SM", "TJ", "TM", "TR", "UA", "UZ", "VA", "IT", "FR", "ES", "IE", "AT", "BE", "DK", "FI", "DE", "GR", "LU", "NL", "PT", "SE", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/na/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}], "channel": {"tegID": "j53t6hsedat4l7rkbb1le98u73262sh5", "__typename": "Content"}, "_metadata": {"articleId": "/content/97rviamdk8sorcopa3sr5botkn06oo7d", "tegID": "97rviamdk8sorcopa3sr5botkn06oo7d", "title": "Artificial intelligence - The world needs an international agency for artificial intelligence, say two AI experts | By Invitation | The Economist", "shareSnippet": "Artificial intelligence \u2013 The world needs an international agency for artificial intelligence, say two AI experts", "headline": "The world needs an international agency for artificial intelligence, say two AI experts", "section": "By Invitation", "keywords": [], "author": ["The Economist"], "url": "https://www.economist.com/by-invitation/2023/04/18/the-world-needs-an-international-agency-for-artificial-intelligence-say-two-ai-experts", "type": "Article", "articleBody": "NEW GENERATIVE-AI tools like OpenAI\u2019s ChatGPT, the fastest-growing consumer internet application of all time, have taken the world by storm. They have uses in everything from education to medicine and are astonishingly fun to play with. Although current AI systems are capable of spectacular feats they also carry risks. Europol has warned that they might greatly increase cybercrime. Many AI experts are deeply worried about their potential to create a tsunami of misinformation, posing an imminent threat to the American presidential election in 2024, and ultimately to democracy itself, by creating an atmosphere of total distrust. Scientists have warned that these new tools could be used to design novel, deadly toxins. Others speculate that in the long term there could be a genuine risk to humanity itself. \nOne of the key issues with current AI systems is that they are primarily black boxes, often unreliable and hard to interpret, and at risk of getting out of control. For example, the core technology underlying systems like ChatGPT, large language models (LLMs), is known to \u201challucinate\u201d, making up false statements. ChatGPT, for example, falsely accused a law professor of being involved in sexual harassment, apparently confused by statistical but irrelevant connections between bits of text that didn\u2019t actually belong together. After an op-ed tried to clarify what had gone wrong, Bing Chat made a similar error, and attributed it to information in USA Today that the chatbot got completely backwards. \nThese systems can also be used for deliberate abuse, from disrupting elections (for example by manipulating what candidates appear to say or write) to spreading medical misinformation. In a recent analysis of GPT-4, OpenAI\u2019s most advanced LLM, the company acknowledged 12 serious concerns\u2014without providing firm solutions to any of them. \nIn the past year alone 37 regulations mentioning AI were passed around the globe; Italy went so far as to ban ChatGPT. But there is little global co-ordination. Even within some countries there is a hodge-podge, such as different state laws in America, or Britain\u2019s proposal to eschew a central regulator, leaving oversight split among several agencies. An uneven, loophole-ridden patchwork is to no one\u2019s benefit and safety. Nor should companies want to build a different AI model for each jurisdiction and face their own de novo struggle to navigate legal, cultural and social contexts. \nRead more from our special series on AI:\nLarge, creative AI models will transform lives and labour markets\nHow generative models could go wrong\nLarge language models\u2019 ability to generate text also lets them plan and reason\n\n\nStill, there is plenty of agreement about basic responsible AI principles, such as safety and reliability, transparency, explainability, interpretability, privacy, accountability and fairness. And almost everyone agrees that something must be done\u2014a just-published poll by the Centre for the Governance of AI found that 91% of a representative sample of 13,000 people across 11 countries agreed that AI needs to be carefully managed. \nIt is in this context that we call for the immediate development of a global, neutral, non-profit International Agency for AI (IAAI), with guidance and buy-in from governments, large technology companies, non-profits, academia and society at large, aimed at collaboratively finding governance and technical solutions to promote safe, secure and peaceful AI technologies. \nThe time for such an agency has come, as Google CEO Sundar Pichai himself said on April 16th. What might that look like? Each domain and each industry will be different, with its own set of guidelines, but many will involve both global governance and technological innovation. For example, people have long agreed that making employment decisions based on gender should be avoided, and have even come up with some measures in earlier, more interpretable AI, such as the interpretability requirements of the AI Bill of Rights proposed by the Biden administration. But in black-box systems like ChatGPT there is a wide variety of use cases with no current remedy. People might, for example, feed in a job candidate\u2019s entire file and ask ChatGPT for a judgment, but we currently have no way to ensure that ChatGPT would avoid bias in its output. The kind of entity we envision would collaboratively address what to do about such \u201coff-label\u201d uses of chatbots and other policy questions, and at the same time develop technical tools for effective auditing. \nThe IAAI could likewise convene experts and develop tools to tackle the spread of misinformation. On the policy side, it could ask, for instance, how wide-scale spreading of misinformation might be penalised. On the technical side, the initial focus should be on developing automated or semi-automated tools for answering fundamental questions, such as \u201cHow much misinformation is out there?\u201d, \u201cHow rapidly is its volume growing?\u201d and \u201cHow much is AI contributing to such problems?\u201d Existing technologies are better at generating misinformation than detecting it. Considerable technical innovation will be required, and of great public benefit, but may or may not be of sufficiently direct commercial interest \u2013 hence the need for independent support by an entity like the IAAI. \nTo take a third, very recent example, systems with names like AutoGPT and BabyAGI have been devised that allow amateurs to build complex and difficult-to-debug (or even fathom) assemblies of unreliable AI systems controlling other unreliable AI systems to achieve arbitrary goals\u2014a practice that may or may not prove to be safe. As Marek Rosa, CEO of GOOD.Ai, put it, we need new technical ideas on \u201chow to increase security (proactive defence) in a world where there are billions of AI agents\u2026running in apps and servers, and we don\u2019t know what they are talking about\u201d, perhaps necessitating a kind of \u201cantivirus [software] against AI agents\u201d. A global alliance with top experts and researchers on call would be able to give swift and thoughtful guidance on such new developments. \nDesigning the kind of global collaboration we envision is an enormous job. Many stakeholders need to be involved. Both short-term and long-term risks must be considered. No solution is going to succeed unless both governments and companies are on board, and it\u2019s not just them: the world\u2019s publics need a seat at the table. \nFortunately, there is precedent for such global co-operation. At the end of the second world war, for example, nuclear weapons sparked deep fears and uncertainties about how the new technology would be used. As a response, 81 countries unanimously approved the International Atomic Energy Agency\u2019s statute to \u201cpromote safe, secure and peaceful nuclear technologies\u201d, with inspection rights. A different, softer kind of model, with less focus on enforcement, is the International Civil Aviation Organisation, in which member countries make their own laws but take counsel from a global agency. Getting to the right model, and making the right choices, will take time, wisdom and collaboration. \nThe challenges and risks of AI are, of course, very different and, to a disconcerting degree, still unknown. We know in hindsight that the internet might have been designed in better ways with more forethought. Earlier decisions about how to handle privacy and anonymity, for instance, might have ensured that there was less of a culture of trolling. We also know that early choices get locked in. Our decisions now are likely to have lasting consequences and must be made thoughtfully.\nGiven how fast things are moving, there is not a lot of time to waste. A global, neutral non-profit with support from governments, big business and society is an important start. \u25a0\nGary Marcus is Emeritus Professor at NYU and was founder and CEO of Geometric Intelligence, a machine-learning company acquired by Uber. Anka Reuel is a PhD student in computer science at Stanford University and founding member of KIRA, a think-tank focusing on the promotion of responsible AI.", "description": "Gary Marcus and Anka Reuel argue that global governance must be prioritised to address the risks of bias, misinformation or worse", "ogDescription": "Gary Marcus and Anka Reuel argue that global governance must be prioritised to address the risks of bias, misinformation or worse", "imageUrl": "https://www.economist.com/media-assets/image/20230418_BID001.jpg", "imageHeight": 720, "imageWidth": 1280, "datePublished": "2023-04-18T20:20:24Z", "dateModified": "2023-04-20T16:59:35Z", "dateCreated": "2023-04-18T20:20:12Z", "isPrintArticle": true, "printEdition": "2023-04-22T00:00:00Z", "copyrightYear": 2023, "dateline": "", "inLanguage": "en", "interactive": false, "scripts": [], "css": []}, "sectionArticles": [{"id": "/content/97rviamdk8sorcopa3sr5botkn06oo7d", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 350.47, "__typename": "Content"}, "__typename": "Content"}], "headline": "The world needs an international agency for artificial intelligence, say two AI experts", "url": {"canonical": "https://www.economist.com/by-invitation/2023/04/18/the-world-needs-an-international-agency-for-artificial-intelligence-say-two-ai-experts", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/o2tq002mahu14nbvv3vok4es8vjgnrs7", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 350.46, "__typename": "Content"}, "__typename": "Content"}], "headline": "Kaja Kallas says Ukraine is giving the free world a masterclass on cyber-defence", "url": {"canonical": "https://www.economist.com/by-invitation/2023/04/17/kaja-kallas-says-ukraine-is-giving-the-free-world-a-masterclass-on-cyber-defence", "__typename": "URL"}, "__typename": "Content"}]}