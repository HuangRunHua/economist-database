{"url": {"canonical": "https://www.economist.com/leaders/2023/04/20/how-to-worry-wisely-about-artificial-intelligence", "__typename": "URL"}, "__typename": "Content", "id": "/content/4ct5r2etoctqg4eqe86ateetnpo4nje3", "tegID": "4ct5r2etoctqg4eqe86ateetnpo4nje3", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "How to worry wisely about artificial intelligence", "subheadline": "Technology and society", "seoPageTitle": null, "seoMetadataDescription": null, "ad": {"grapeshot": {"channels": [{"name": "gv_safe", "score": 23680.396, "__typename": "GrapeshotChannel"}, {"name": "gt_mixed", "score": 4.359, "__typename": "GrapeshotChannel"}, {"name": "future_of_work_test", "score": 4.095, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_computing", "score": 3.486, "__typename": "GrapeshotChannel"}, {"name": "chanel_neg", "score": 2.962, "__typename": "GrapeshotChannel"}, {"name": "gs_tech", "score": 2.654, "__typename": "GrapeshotChannel"}, {"name": "ibm_blacklist", "score": 2.391, "__typename": "GrapeshotChannel"}, {"name": "artificial_intelligence", "score": 2.303, "__typename": "GrapeshotChannel"}, {"name": "neg_google_youtube_2020", "score": 2.282, "__typename": "GrapeshotChannel"}, {"name": "ibm_cloud", "score": 2.151, "__typename": "GrapeshotChannel"}, {"name": "neg_omd_exclusion", "score": 2.106, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin_business", "score": 2.098, "__typename": "GrapeshotChannel"}, {"name": "america_nanny_state", "score": 1.951, "__typename": "GrapeshotChannel"}, {"name": "neg_3166_vca_brand-safety", "score": 1.861, "__typename": "GrapeshotChannel"}, {"name": "neg_3166_vca_brand-safety3", "score": 1.861, "__typename": "GrapeshotChannel"}, {"name": "gt_positive_curiosity", "score": 1.711, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin", "score": 1.708, "__typename": "GrapeshotChannel"}, {"name": "fow_barclays", "score": 1.707, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_ai", "score": 1.681, "__typename": "GrapeshotChannel"}, {"name": "neg_facebook", "score": 1.64, "__typename": "GrapeshotChannel"}, {"name": "gs_science_misc", "score": 1.629, "__typename": "GrapeshotChannel"}, {"name": "cigna_healthyhybridworkplace", "score": 1.6, "__typename": "GrapeshotChannel"}, {"name": "custom_punkt", "score": 1.468, "__typename": "GrapeshotChannel"}, {"name": "google_negative_keywords", "score": 1.445, "__typename": "GrapeshotChannel"}, {"name": "neg_morgan_stanley_2019_neg_keywords", "score": 1.379, "__typename": "GrapeshotChannel"}, {"name": "ts_tech", "score": 1.318, "__typename": "GrapeshotChannel"}, {"name": "gt_negative_fear", "score": 1.295, "__typename": "GrapeshotChannel"}, {"name": "gs_business", "score": 1.266, "__typename": "GrapeshotChannel"}, {"name": "test", "score": 1.239, "__typename": "GrapeshotChannel"}, {"name": "neg_huawei_brandsafety", "score": 1.188, "__typename": "GrapeshotChannel"}, {"name": "neg_dit4", "score": 1.127, "__typename": "GrapeshotChannel"}, {"name": "neg_exxon", "score": 1.079, "__typename": "GrapeshotChannel"}, {"name": "gs_science", "score": 1.077, "__typename": "GrapeshotChannel"}, {"name": "future_of_work", "score": 0.992, "__typename": "GrapeshotChannel"}, {"name": "gs_genres", "score": 0.979, "__typename": "GrapeshotChannel"}, {"name": "microsoft_blacklist", "score": 0.975, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_robotics", "score": 0.973, "__typename": "GrapeshotChannel"}, {"name": "asia_nec_technology", "score": 0.969, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin_indus", "score": 0.963, "__typename": "GrapeshotChannel"}, {"name": "neg_dit", "score": 0.924, "__typename": "GrapeshotChannel"}, {"name": "ge_tech_enthusiasts", "score": 0.907, "__typename": "GrapeshotChannel"}, {"name": "america_department_commerce", "score": 0.874, "__typename": "GrapeshotChannel"}, {"name": "neg_ibm_brandsafety", "score": 0.861, "__typename": "GrapeshotChannel"}, {"name": "gs_tech_compute_net", "score": 0.853, "__typename": "GrapeshotChannel"}, {"name": "business_it_decisionmakers", "score": 0.83, "__typename": "GrapeshotChannel"}, {"name": "america_department_security", "score": 0.812, "__typename": "GrapeshotChannel"}, {"name": "progressivemedia", "score": 0.809, "__typename": "GrapeshotChannel"}, {"name": "gs_fineart", "score": 0.783, "__typename": "GrapeshotChannel"}, {"name": "business_engineering", "score": 0.78, "__typename": "GrapeshotChannel"}, {"name": "workdayblocklist", "score": 0.774, "__typename": "GrapeshotChannel"}, {"name": "fidelity_blacklist", "score": 0.756, "__typename": "GrapeshotChannel"}, {"name": "eu_general", "score": 0.748, "__typename": "GrapeshotChannel"}, {"name": "neg_ey_brandsafety", "score": 0.733, "__typename": "GrapeshotChannel"}, {"name": "neg_ey_kwbl", "score": 0.733, "__typename": "GrapeshotChannel"}, {"name": "gs_busfin_business_startups", "score": 0.727, "__typename": "GrapeshotChannel"}, {"name": "custom_punkt_design", "score": 0.709, "__typename": "GrapeshotChannel"}, {"name": "gs_politics", "score": 0.708, "__typename": "GrapeshotChannel"}, {"name": "gs_politics_issues_policy", "score": 0.708, "__typename": "GrapeshotChannel"}, {"name": "gs_politics_misc", "score": 0.708, "__typename": "GrapeshotChannel"}, {"name": "vca_brand_safety", "score": 0.706, "__typename": "GrapeshotChannel"}], "__typename": "Grapeshot"}, "__typename": "Ad"}, "audio": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/audio/005%20Leaders%20-%20Artificial%20intelligence-be82051618ed4a9eb80a325ccba89648.mp3", "__typename": "URL"}, "__typename": "Content"}, "__typename": "Media"}, "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_LDD001.jpg", "__typename": "URL"}, "__typename": "Content", "height": 720, "width": 1280, "description": ""}, "promo": null, "__typename": "Media"}, "description": "Rapid progress in AI is arousing fear as well as excitement. How worried should you be?", "datePublished": "2023-04-20T09:41:17Z", "dateModified": "2023-04-20T17:14:19Z", "dateModifiedString": "Apr 20th 2023", "datePublishedString": "Apr 20th 2023", "dateCreated": "2023-04-19T15:17:41Z", "copyrightYear": 2023, "inLanguage": "en", "byline": "", "dateline": null, "text": [{"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "span", "attribs": {"data-caps": "initial"}, "children": [{"data": "\u201cS", "type": "text"}]}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "hould we automate", "type": "text"}]}, {"data": " away all the jobs, including the fulfilling ones? Should we develop non-human minds that might eventually outnumber, outsmart...and replace us? Should we risk loss of control of our civilisation?\u201d These questions were asked last month in an open letter from the Future of Life Institute, an ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "ngo", "type": "text"}]}, {"data": ". It called for a six-month \u201cpause\u201d in the creation of the most advanced forms of artificial intelligence (", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "), and was signed by tech luminaries including Elon Musk. It is the most prominent example yet of how rapid progress in ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/business/2023/03/06/dont-fear-an-ai-induced-jobs-apocalypse-just-yet"}, "children": [{"data": "has sparked anxiety", "type": "text"}]}, {"data": " about the potential dangers of the technology.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "In particular, new \u201clarge language models\u201d (", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s)\u2014the sort that powers ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/graphic-detail/2023/04/14/chatgpt-could-replace-telemarketers-teachers-and-traders"}, "children": [{"data": "Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}]}, {"data": ", a chatbot made by Open", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ", a startup\u2014have surprised even their creators with their unexpected talents as they have been scaled up. Such \u201cemergent\u201d abilities include everything from solving logic puzzles and writing computer code to identifying films from plot summaries written in emoji. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "These models stand to ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/essay/2023/04/20/how-ai-could-change-computing-culture-and-the-course-of-history", "data-tegid": "re6vv2c3kp32g0823hf6hmtqli1qnvas"}, "children": [{"data": "transform humans\u2019 relationship", "type": "text"}]}, {"data": " with computers, knowledge and even with themselves. Proponents of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " argue for its potential to solve big problems by developing new drugs, designing new materials to help fight climate change, or untangling the complexities of fusion power. To others, the fact that ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "ai", "type": "text"}]}, {"data": "s\u2019 capabilities are already outrunning their creators\u2019 understanding risks bringing to life the science-fiction disaster scenario of the machine that outsmarts its inventor, often with fatal consequences.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "This bubbling mixture of excitement and fear makes it hard to weigh the opportunities and risks. But lessons can be learned from other industries, and from past technological shifts. So what has changed to make ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " so much more capable? How scared should you be? And what should governments do?", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "In a ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/interactive/science-and-technology/2023/04/22/large-creative-ai-models-will-transform-how-we-live-and-work"}, "children": [{"data": "special Science section", "type": "text"}]}, {"data": ", we explore the workings of", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": " llm", "type": "text"}]}, {"data": "s and their future direction. The first wave of modern ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems, which emerged a decade ago, relied on carefully labelled training data. Once exposed to a sufficient number of labelled examples, they could learn to do things like recognise images or transcribe speech. Today\u2019s systems do not require pre-labelling, and as a result can be trained using much larger data sets taken from online sources. ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s can, in effect, be trained on the entire internet\u2014which explains their capabilities, good and bad.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Those capabilities became apparent to a wider public when Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": " was released in November. A million people had used it within a week; 100m within two months. It was soon being used to generate school essays and wedding speeches. Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": "\u2019s popularity, and Microsoft\u2019s move to incorporate it into Bing, its search engine, prompted rival firms to release chatbots too. ", "type": "text"}]}, {"type": "tag", "name": "aside", "attribs": {}, "children": [{"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Read more of our special series on ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": ": ", "type": "text"}]}, {"type": "tag", "name": "ul", "attribs": {}, "children": [{"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/interactive/science-and-technology/2023/04/22/large-creative-ai-models-will-transform-how-we-live-and-work"}, "children": [{"data": "Large, creative ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " models will transform lives and labour markets", "type": "text"}]}]}, {"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/science-and-technology/2023/04/19/large-language-models-ability-to-generate-text-also-lets-them-plan-and-reason"}, "children": [{"data": "Large language models\u2019 ability to generate text also lets them plan and reason", "type": "text"}]}]}, {"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/science-and-technology/2023/04/19/how-generative-models-could-go-wrong"}, "children": [{"data": "How generative models could go wrong", "type": "text"}]}]}, {"type": "tag", "name": "li", "attribs": {}, "children": [{"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/by-invitation/2023/04/18/the-world-needs-an-international-agency-for-artificial-intelligence-say-two-ai-experts"}, "children": [{"data": "The world needs an international agency for artificial intelligence, say two ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " experts", "type": "text"}]}]}]}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Some of these produced strange results. Bing Chat suggested to a journalist that he should leave his wife. Chat", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "GPT", "type": "text"}]}, {"data": " has been accused of defamation by a law professor. ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s produce answers that have the patina of truth, but often contain factual errors or outright fabrications. Even so, Microsoft, Google and other tech firms have begun to incorporate ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s into their products, to help users create documents and perform other tasks. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The recent acceleration in both the power and visibility of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems, and growing awareness of their abilities and defects, have raised fears that the technology is now advancing so quickly that it cannot be safely controlled. Hence the call for a pause, and growing concern that ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " could threaten not just jobs, factual accuracy and reputations, but the existence of humanity itself. ", "type": "text"}]}, {"type": "tag", "name": "h2", "attribs": {}, "children": [{"data": "Extinction? Rebellion?", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The fear that machines will steal jobs is centuries old. But so far new technology has created new jobs to replace the ones it has destroyed. Machines tend to be able to perform some tasks, not others, increasing demand for people who can do the jobs machines cannot. Could this time be different? A sudden dislocation in job markets cannot be ruled out, even if so far there is ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/business/2023/04/19/how-businesses-are-experimenting-with-chatgpt-like-services", "data-tegid": "ioej1k6q4o860jr54hmdi80ue3fcni9n"}, "children": [{"data": "no sign of one", "type": "text"}]}, {"data": ". Previous technology has tended to replace unskilled tasks, but ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "LLM", "type": "text"}]}, {"data": "s can perform some white-collar tasks, such as summarising documents and writing code.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The degree of existential risk posed by ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " has been hotly debated. Experts are divided. In a survey of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " researchers carried out in 2022, 48% thought there was at least a 10% chance that ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "\u2019s impact would be \u201cextremely bad (eg, human extinction)\u201d. But 25% said the risk was 0%; the median researcher put the risk at 5%. The nightmare is that an advanced ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " causes harm on a massive scale, by making poisons or viruses, or persuading humans to commit terrorist acts. It need not have evil intent: researchers worry that future ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "s may have goals that do not align with those of their human creators.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Such scenarios should not be dismissed. But all involve a huge amount of guesswork, and a leap from today\u2019s technology. And many imagine that future ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "s will have unfettered access to energy, money and computing power, which are real constraints today, and could be denied to a rogue ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " in future. Moreover, experts tend to overstate the risks in their area, compared with other forecasters. (And Mr Musk, who is launching his own ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " startup, has an interest in his rivals downing tools.) Imposing heavy regulation, or indeed a pause, today seems an over-reaction. A pause would also be unenforceable. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "Regulation is needed, but for more mundane reasons than saving humanity. Existing ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems raise real concerns about bias, privacy and intellectual-property rights. As the technology", "type": "text"}, {"data": " advances, other problems could become apparent. The key is to balance the promise of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " with an assessment of the risks, and to be ready to adapt. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "So far governments are taking three different approaches. At one end of the spectrum is Britain, which has proposed a \u201clight-touch\u201d approach with no new rules or regulatory bodies, but applies existing regulations to ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " systems. The aim is to boost investment and turn Britain into an \u201c", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " superpower\u201d. America has taken a similar approach, though the Biden administration is now seeking public views on what a rulebook might look like.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "The ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "eu ", "type": "text"}]}, {"data": "is taking a tougher line. Its proposed law categorises different uses of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " by the degree of risk, and requires increasingly stringent monitoring and disclosure as the degree of risk rises from, say, music-recommendation to self-driving cars. Some uses of ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " are banned altogether, such as subliminal advertising and remote biometrics. Firms that break the rules will be fined. For some critics, these regulations are too stifling. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "But others say an even sterner approach is needed. Governments should treat ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " like medicines, with a dedicated regulator, strict testing and pre-approval before public release. China is doing some of this, requiring firms to register ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " products and undergo a security review before release. But safety may be less of a motive than politics: a key ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/china/2023/04/18/can-xi-jinping-control-ai-without-crushing-it", "data-tegid": "0f5t992jh5a1ontne5t1kq9vq31cpc5a"}, "children": [{"data": "requirement", "type": "text"}]}, {"data": " is that ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": "s\u2019 output reflects the \u201ccore value of socialism\u201d.", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "figure", "attribs": {"itemtype": "https://schema.org/MediaObject", "class": "op-interactive"}, "children": [{"type": "tag", "name": "iframe", "attribs": {"src": "https://www.youtube.com/embed/ANn9ibNo9SQ", "height": "360", "width": "640"}, "children": []}]}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "What to do? The light-touch approach is unlikely to be enough. If ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " is as important a technology as cars, planes and medicines\u2014and there is good reason to believe that it is\u2014then, like them, it will need new rules. Accordingly, the ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "EU", "type": "text"}]}, {"data": "\u2019s model is closest to the mark, though its classification system is overwrought and a principles-based approach would be more flexible. Compelling disclosure about how systems are trained, how they operate and how they are monitored, and requiring inspections, would be comparable to similar rules in other industries. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "This could allow for tighter regulation over time, if needed. A dedicated regulator may then seem appropriate; so too may intergovernmental treaties, similar to those that govern nuclear weapons, should plausible evidence emerge of existential risk. To monitor that risk, governments could form a body modelled on ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "CERN", "type": "text"}]}, {"data": ", a particle-physics laboratory, that could also study ", "type": "text"}, {"type": "tag", "name": "small", "attribs": {}, "children": [{"data": "AI", "type": "text"}]}, {"data": " safety and ethics\u2014areas where companies lack incentives to invest as much as society might wish. ", "type": "text"}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"data": "This powerful technology poses new risks, but also offers extraordinary opportunities. Balancing the two means treading carefully. A measured approach today can provide the foundations on which further rules can be added in future. But the time to start building those foundations is now. ", "type": "text"}, {"type": "tag", "name": "span", "attribs": {"data-ornament": "ufinish"}, "children": [{"data": "\u25a0", "type": "text"}]}]}, {"type": "tag", "name": "p", "attribs": {}, "children": [{"type": "tag", "name": "i", "attribs": {}, "children": [{"data": "For subscribers only: to see how we design each week\u2019s cover, sign up to our weekly ", "type": "text"}, {"type": "tag", "name": "a", "attribs": {"href": "https://www.economist.com/newsletters/cover-story"}, "children": [{"data": "Cover Story newsletter.", "type": "text"}]}]}]}], "bodyText": "\u201cShould we automate away all the jobs, including the fulfilling ones? Should we develop non-human minds that might eventually outnumber, outsmart...and replace us? Should we risk loss of control of our civilisation?\u201d These questions were asked last month in an open letter from the Future of Life Institute, an ngo. It called for a six-month \u201cpause\u201d in the creation of the most advanced forms of artificial intelligence (AI), and was signed by tech luminaries including Elon Musk. It is the most prominent example yet of how rapid progress in AI has sparked anxiety about the potential dangers of the technology.\nIn particular, new \u201clarge language models\u201d (LLMs)\u2014the sort that powers ChatGPT, a chatbot made by OpenAI, a startup\u2014have surprised even their creators with their unexpected talents as they have been scaled up. Such \u201cemergent\u201d abilities include everything from solving logic puzzles and writing computer code to identifying films from plot summaries written in emoji. \nThese models stand to transform humans\u2019 relationship with computers, knowledge and even with themselves. Proponents of AI argue for its potential to solve big problems by developing new drugs, designing new materials to help fight climate change, or untangling the complexities of fusion power. To others, the fact that ais\u2019 capabilities are already outrunning their creators\u2019 understanding risks bringing to life the science-fiction disaster scenario of the machine that outsmarts its inventor, often with fatal consequences.\nThis bubbling mixture of excitement and fear makes it hard to weigh the opportunities and risks. But lessons can be learned from other industries, and from past technological shifts. So what has changed to make AI so much more capable? How scared should you be? And what should governments do?\nIn a special Science section, we explore the workings of llms and their future direction. The first wave of modern AI systems, which emerged a decade ago, relied on carefully labelled training data. Once exposed to a sufficient number of labelled examples, they could learn to do things like recognise images or transcribe speech. Today\u2019s systems do not require pre-labelling, and as a result can be trained using much larger data sets taken from online sources. LLMs can, in effect, be trained on the entire internet\u2014which explains their capabilities, good and bad.\nThose capabilities became apparent to a wider public when ChatGPT was released in November. A million people had used it within a week; 100m within two months. It was soon being used to generate school essays and wedding speeches. ChatGPT\u2019s popularity, and Microsoft\u2019s move to incorporate it into Bing, its search engine, prompted rival firms to release chatbots too. \nRead more of our special series on AI: \nLarge, creative AI models will transform lives and labour markets\nLarge language models\u2019 ability to generate text also lets them plan and reason\nHow generative models could go wrong\nThe world needs an international agency for artificial intelligence, say two AI experts\n\n\nSome of these produced strange results. Bing Chat suggested to a journalist that he should leave his wife. ChatGPT has been accused of defamation by a law professor. LLMs produce answers that have the patina of truth, but often contain factual errors or outright fabrications. Even so, Microsoft, Google and other tech firms have begun to incorporate LLMs into their products, to help users create documents and perform other tasks. \nThe recent acceleration in both the power and visibility of AI systems, and growing awareness of their abilities and defects, have raised fears that the technology is now advancing so quickly that it cannot be safely controlled. Hence the call for a pause, and growing concern that AI could threaten not just jobs, factual accuracy and reputations, but the existence of humanity itself. \nExtinction? Rebellion?\nThe fear that machines will steal jobs is centuries old. But so far new technology has created new jobs to replace the ones it has destroyed. Machines tend to be able to perform some tasks, not others, increasing demand for people who can do the jobs machines cannot. Could this time be different? A sudden dislocation in job markets cannot be ruled out, even if so far there is no sign of one. Previous technology has tended to replace unskilled tasks, but LLMs can perform some white-collar tasks, such as summarising documents and writing code.\nThe degree of existential risk posed by AI has been hotly debated. Experts are divided. In a survey of AI researchers carried out in 2022, 48% thought there was at least a 10% chance that AI\u2019s impact would be \u201cextremely bad (eg, human extinction)\u201d. But 25% said the risk was 0%; the median researcher put the risk at 5%. The nightmare is that an advanced AI causes harm on a massive scale, by making poisons or viruses, or persuading humans to commit terrorist acts. It need not have evil intent: researchers worry that future AIs may have goals that do not align with those of their human creators.\nSuch scenarios should not be dismissed. But all involve a huge amount of guesswork, and a leap from today\u2019s technology. And many imagine that future AIs will have unfettered access to energy, money and computing power, which are real constraints today, and could be denied to a rogue AI in future. Moreover, experts tend to overstate the risks in their area, compared with other forecasters. (And Mr Musk, who is launching his own AI startup, has an interest in his rivals downing tools.) Imposing heavy regulation, or indeed a pause, today seems an over-reaction. A pause would also be unenforceable. \nRegulation is needed, but for more mundane reasons than saving humanity. Existing AI systems raise real concerns about bias, privacy and intellectual-property rights. As the technology advances, other problems could become apparent. The key is to balance the promise of AI with an assessment of the risks, and to be ready to adapt. \nSo far governments are taking three different approaches. At one end of the spectrum is Britain, which has proposed a \u201clight-touch\u201d approach with no new rules or regulatory bodies, but applies existing regulations to AI systems. The aim is to boost investment and turn Britain into an \u201cAI superpower\u201d. America has taken a similar approach, though the Biden administration is now seeking public views on what a rulebook might look like.\nThe eu is taking a tougher line. Its proposed law categorises different uses of AI by the degree of risk, and requires increasingly stringent monitoring and disclosure as the degree of risk rises from, say, music-recommendation to self-driving cars. Some uses of AI are banned altogether, such as subliminal advertising and remote biometrics. Firms that break the rules will be fined. For some critics, these regulations are too stifling. \nBut others say an even sterner approach is needed. Governments should treat AI like medicines, with a dedicated regulator, strict testing and pre-approval before public release. China is doing some of this, requiring firms to register AI products and undergo a security review before release. But safety may be less of a motive than politics: a key requirement is that AIs\u2019 output reflects the \u201ccore value of socialism\u201d.\n\nWhat to do? The light-touch approach is unlikely to be enough. If AI is as important a technology as cars, planes and medicines\u2014and there is good reason to believe that it is\u2014then, like them, it will need new rules. Accordingly, the EU\u2019s model is closest to the mark, though its classification system is overwrought and a principles-based approach would be more flexible. Compelling disclosure about how systems are trained, how they operate and how they are monitored, and requiring inspections, would be comparable to similar rules in other industries. \nThis could allow for tighter regulation over time, if needed. A dedicated regulator may then seem appropriate; so too may intergovernmental treaties, similar to those that govern nuclear weapons, should plausible evidence emerge of existential risk. To monitor that risk, governments could form a body modelled on CERN, a particle-physics laboratory, that could also study AI safety and ethics\u2014areas where companies lack incentives to invest as much as society might wish. \nThis powerful technology poses new risks, but also offers extraordinary opportunities. Balancing the two means treading carefully. A measured approach today can provide the foundations on which further rules can be added in future. But the time to start building those foundations is now. \u25a0\nFor subscribers only: to see how we design each week\u2019s cover, sign up to our weekly Cover Story newsletter.", "about": {"public": null, "__typename": "Taxonomies"}, "print": {"headline": "How to worry wisely about AI", "section": {"url": {"canonical": "https://www.economist.com/leaders/", "__typename": "URL"}, "__typename": "Content", "headline": "Leaders"}, "__typename": "Print"}, "articleSection": {"public": null, "internal": [{"url": {"canonical": "https://www.economist.com/leaders/", "__typename": "URL"}, "__typename": "Content", "id": "/content/0eqdc3kshoq96naup34ruco7pktc6n51", "tegID": "0eqdc3kshoq96naup34ruco7pktc6n51", "headline": "Leaders", "hasPart": {"parts": [{"url": {"canonical": "https://www.economist.com/leaders/2023/04/20/ukraines-coming-counter-offensive-may-shape-its-future-and-europes", "__typename": "URL"}, "__typename": "Content", "id": "/content/tllsauohlcqsej2f7lld8rutcb40qe50", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Ukraine\u2019s coming counter-offensive may shape its future\u2014and Europe\u2019s", "subheadline": "Waiting for the order", "datePublished": "2023-04-20T15:11:14Z", "description": "It will set the scene for any future peace talks", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_LDP001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/leaders/2023/04/20/bolivias-crisis-shows-the-limits-of-left-wing-populism", "__typename": "URL"}, "__typename": "Content", "id": "/content/2vipu3l1c8g7u5v3tg124qhj2smlhe6j", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Bolivia\u2019s crisis shows the limits of left-wing populism", "subheadline": "On the brink", "datePublished": "2023-04-20T15:11:14Z", "description": "The country is running out of money. It should serve as a warning to Latin America", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_LDP503.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/leaders/2023/04/20/why-the-world-should-welcome-competition-from-chinese-carmakers", "__typename": "URL"}, "__typename": "Content", "id": "/content/aap14j8gd5hhnncgg26ruom74a5ef0rb", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Why the world should welcome competition from Chinese carmakers", "subheadline": "Great wheels from China", "datePublished": "2023-04-20T15:11:14Z", "description": "Deglobalisation would be bad for drivers and the planet", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_LDD002.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/leaders/2023/04/20/how-to-worry-wisely-about-artificial-intelligence", "__typename": "URL"}, "__typename": "Content", "id": "/content/4ct5r2etoctqg4eqe86ateetnpo4nje3", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "How to worry wisely about artificial intelligence", "subheadline": "Technology and society", "datePublished": "2023-04-20T09:41:17Z", "description": "Rapid progress in AI is arousing fear as well as excitement. How worried should you be?", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_LDD001.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}, {"url": {"canonical": "https://www.economist.com/leaders/2023/04/20/why-america-will-soon-see-a-wave-of-bank-mergers", "__typename": "URL"}, "__typename": "Content", "id": "/content/ld3j5vq3je2190ctq11btf3paahu4f7g", "type": ["Article", "NewsArticle", "AnalysisNewsArticle"], "headline": "Why America will soon see a wave of bank mergers", "subheadline": "The prize of size", "datePublished": "2023-04-20T09:39:50Z", "description": "Cheap valuations and a stricter rulebook point towards more consolidation", "image": {"main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_LDP504.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 720}, "__typename": "Media", "promo": null}}], "__typename": "HasPart"}}], "__typename": "Taxonomies"}, "publication": [{"url": {"canonical": "https://www.economist.com/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_AP.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["UK", "GB", "GG", "IM", "JE", "AD", "AL", "AM", "AX", "AZ", "BA", "BG", "BY", "CH", "CY", "CZ", "EE", "FO", "GE", "GI", "GL", "HR", "HU", "IS", "KG", "KZ", "LI", "LT", "LV", "MC", "MD", "ME", "MK", "MT", "NO", "PL", "RO", "RS", "RU", "SI", "SJ", "SK", "SM", "TJ", "TM", "TR", "UA", "UZ", "VA", "IT", "FR", "ES", "IE", "AT", "BE", "DK", "FI", "DE", "GR", "LU", "NL", "PT", "SE", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_AP.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}, {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/eu/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/ap/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_AP.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["AF", "AS", "AU", "BD", "BT", "BN", "KH", "CN", "CK", "FJ", "GU", "PF", "HK", "IN", "ID", "JP", "KI", "KP", "KR", "LA", "MO", "MY", "MV", "MH", "FM", "MN", "MM", "NR", "NP", "NC", "NZ", "PK", "PG", "PH", "PN", "SC", "SG", "SB", "LK", "TF", "TW", "TH", "TO", "TV", "VU", "VN", "IO", "CC", "TL", "HM", "NU", "NF", "MP", "PW", "WS", "CX", "TK", "WF"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_AP.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_AP.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/la/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/me/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/uk/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["UK", "GB", "GG", "IM", "JE", "AD", "AL", "AM", "AX", "AZ", "BA", "BG", "BY", "CH", "CY", "CZ", "EE", "FO", "GE", "GI", "GL", "HR", "HU", "IS", "KG", "KZ", "LI", "LT", "LV", "MC", "MD", "ME", "MK", "MT", "NO", "PL", "RO", "RS", "RU", "SI", "SJ", "SK", "SM", "TJ", "TM", "TR", "UA", "UZ", "VA", "IT", "FR", "ES", "IE", "AT", "BE", "DK", "FI", "DE", "GR", "LU", "NL", "PT", "SE", "BH", "IR", "IQ", "IL", "JO", "KW", "LB", "OM", "SA", "SY", "AE", "PS", "QA", "YE", "DZ", "AO", "BJ", "BW", "BF", "BI", "CM", "CV", "CF", "TD", "KM", "CD", "CI", "DJ", "EG", "GQ", "ER", "ET", "GA", "GM", "GH", "GN", "GW", "KE", "LS", "LR", "LY", "MG", "MU", "MW", "ML", "MR", "YT", "MA", "MZ", "NA", "NE", "NG", "RE", "RW", "SH", "SS", "ST", "SN", "SL", "SO", "ZA", "SD", "SZ", "TZ", "TG", "TN", "UG", "EH", "ZM", "ZW", "CG"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_UK.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}, {"url": {"canonical": "https://www.economist.com/na/printedition/2023-04-22", "__typename": "URL"}, "__typename": "Content", "type": ["PublicationIssue", "RegionalIssue"], "headline": "WEEKLY EDITION: APR 22ND 2023", "description": "", "subheadline": "", "datePublished": "2023-04-22T00:00:00Z", "datePublishedString": "Apr 22nd 2023", "id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "image": {"cover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "headline": "How to worry wisely about AI", "width": 1280, "height": 1684, "regionsAllowed": ["US", "CA", "PM", "UM", "AI", "BL", "BQ", "BV", "CW", "GS", "MF", "SX", "AG", "AR", "AW", "BS", "BB", "BZ", "BM", "BO", "BR", "KY", "CL", "CO", "CR", "CU", "DM", "DO", "EC", "SV", "FK", "GF", "GD", "GP", "GT", "GY", "HN", "HT", "JM", "MQ", "MX", "MS", "NI", "PA", "PY", "PE", "PR", "KN", "LC", "VC", "SR", "TT", "TC", "UY", "VE", "VG", "VI"]}], "main": {"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1684}, "squareCover": [{"url": {"canonical": "https://www.economist.com/media-assets/image/20230422_DE_SQ_US.jpg", "__typename": "URL"}, "__typename": "Content", "width": 1280, "height": 1280}], "__typename": "Media"}}], "channel": {"tegID": "j53t6hsedat4l7rkbb1le98u73262sh5", "__typename": "Content"}, "_metadata": {"articleId": "/content/4ct5r2etoctqg4eqe86ateetnpo4nje3", "tegID": "4ct5r2etoctqg4eqe86ateetnpo4nje3", "title": "Technology and society - How to worry wisely about artificial intelligence | Leaders | The Economist", "shareSnippet": "Technology and society \u2013 How to worry wisely about artificial intelligence", "headline": "How to worry wisely about artificial intelligence", "section": "Leaders", "keywords": [], "author": ["The Economist"], "url": "https://www.economist.com/leaders/2023/04/20/how-to-worry-wisely-about-artificial-intelligence", "type": "Article", "articleBody": "\u201cShould we automate away all the jobs, including the fulfilling ones? Should we develop non-human minds that might eventually outnumber, outsmart...and replace us? Should we risk loss of control of our civilisation?\u201d These questions were asked last month in an open letter from the Future of Life Institute, an ngo. It called for a six-month \u201cpause\u201d in the creation of the most advanced forms of artificial intelligence (AI), and was signed by tech luminaries including Elon Musk. It is the most prominent example yet of how rapid progress in AI has sparked anxiety about the potential dangers of the technology.\nIn particular, new \u201clarge language models\u201d (LLMs)\u2014the sort that powers ChatGPT, a chatbot made by OpenAI, a startup\u2014have surprised even their creators with their unexpected talents as they have been scaled up. Such \u201cemergent\u201d abilities include everything from solving logic puzzles and writing computer code to identifying films from plot summaries written in emoji. \nThese models stand to transform humans\u2019 relationship with computers, knowledge and even with themselves. Proponents of AI argue for its potential to solve big problems by developing new drugs, designing new materials to help fight climate change, or untangling the complexities of fusion power. To others, the fact that ais\u2019 capabilities are already outrunning their creators\u2019 understanding risks bringing to life the science-fiction disaster scenario of the machine that outsmarts its inventor, often with fatal consequences.\nThis bubbling mixture of excitement and fear makes it hard to weigh the opportunities and risks. But lessons can be learned from other industries, and from past technological shifts. So what has changed to make AI so much more capable? How scared should you be? And what should governments do?\nIn a special Science section, we explore the workings of llms and their future direction. The first wave of modern AI systems, which emerged a decade ago, relied on carefully labelled training data. Once exposed to a sufficient number of labelled examples, they could learn to do things like recognise images or transcribe speech. Today\u2019s systems do not require pre-labelling, and as a result can be trained using much larger data sets taken from online sources. LLMs can, in effect, be trained on the entire internet\u2014which explains their capabilities, good and bad.\nThose capabilities became apparent to a wider public when ChatGPT was released in November. A million people had used it within a week; 100m within two months. It was soon being used to generate school essays and wedding speeches. ChatGPT\u2019s popularity, and Microsoft\u2019s move to incorporate it into Bing, its search engine, prompted rival firms to release chatbots too. \nRead more of our special series on AI: \nLarge, creative AI models will transform lives and labour markets\nLarge language models\u2019 ability to generate text also lets them plan and reason\nHow generative models could go wrong\nThe world needs an international agency for artificial intelligence, say two AI experts\n\n\nSome of these produced strange results. Bing Chat suggested to a journalist that he should leave his wife. ChatGPT has been accused of defamation by a law professor. LLMs produce answers that have the patina of truth, but often contain factual errors or outright fabrications. Even so, Microsoft, Google and other tech firms have begun to incorporate LLMs into their products, to help users create documents and perform other tasks. \nThe recent acceleration in both the power and visibility of AI systems, and growing awareness of their abilities and defects, have raised fears that the technology is now advancing so quickly that it cannot be safely controlled. Hence the call for a pause, and growing concern that AI could threaten not just jobs, factual accuracy and reputations, but the existence of humanity itself. \nExtinction? Rebellion?\nThe fear that machines will steal jobs is centuries old. But so far new technology has created new jobs to replace the ones it has destroyed. Machines tend to be able to perform some tasks, not others, increasing demand for people who can do the jobs machines cannot. Could this time be different? A sudden dislocation in job markets cannot be ruled out, even if so far there is no sign of one. Previous technology has tended to replace unskilled tasks, but LLMs can perform some white-collar tasks, such as summarising documents and writing code.\nThe degree of existential risk posed by AI has been hotly debated. Experts are divided. In a survey of AI researchers carried out in 2022, 48% thought there was at least a 10% chance that AI\u2019s impact would be \u201cextremely bad (eg, human extinction)\u201d. But 25% said the risk was 0%; the median researcher put the risk at 5%. The nightmare is that an advanced AI causes harm on a massive scale, by making poisons or viruses, or persuading humans to commit terrorist acts. It need not have evil intent: researchers worry that future AIs may have goals that do not align with those of their human creators.\nSuch scenarios should not be dismissed. But all involve a huge amount of guesswork, and a leap from today\u2019s technology. And many imagine that future AIs will have unfettered access to energy, money and computing power, which are real constraints today, and could be denied to a rogue AI in future. Moreover, experts tend to overstate the risks in their area, compared with other forecasters. (And Mr Musk, who is launching his own AI startup, has an interest in his rivals downing tools.) Imposing heavy regulation, or indeed a pause, today seems an over-reaction. A pause would also be unenforceable. \nRegulation is needed, but for more mundane reasons than saving humanity. Existing AI systems raise real concerns about bias, privacy and intellectual-property rights. As the technology advances, other problems could become apparent. The key is to balance the promise of AI with an assessment of the risks, and to be ready to adapt. \nSo far governments are taking three different approaches. At one end of the spectrum is Britain, which has proposed a \u201clight-touch\u201d approach with no new rules or regulatory bodies, but applies existing regulations to AI systems. The aim is to boost investment and turn Britain into an \u201cAI superpower\u201d. America has taken a similar approach, though the Biden administration is now seeking public views on what a rulebook might look like.\nThe eu is taking a tougher line. Its proposed law categorises different uses of AI by the degree of risk, and requires increasingly stringent monitoring and disclosure as the degree of risk rises from, say, music-recommendation to self-driving cars. Some uses of AI are banned altogether, such as subliminal advertising and remote biometrics. Firms that break the rules will be fined. For some critics, these regulations are too stifling. \nBut others say an even sterner approach is needed. Governments should treat AI like medicines, with a dedicated regulator, strict testing and pre-approval before public release. China is doing some of this, requiring firms to register AI products and undergo a security review before release. But safety may be less of a motive than politics: a key requirement is that AIs\u2019 output reflects the \u201ccore value of socialism\u201d.\n\nWhat to do? The light-touch approach is unlikely to be enough. If AI is as important a technology as cars, planes and medicines\u2014and there is good reason to believe that it is\u2014then, like them, it will need new rules. Accordingly, the EU\u2019s model is closest to the mark, though its classification system is overwrought and a principles-based approach would be more flexible. Compelling disclosure about how systems are trained, how they operate and how they are monitored, and requiring inspections, would be comparable to similar rules in other industries. \nThis could allow for tighter regulation over time, if needed. A dedicated regulator may then seem appropriate; so too may intergovernmental treaties, similar to those that govern nuclear weapons, should plausible evidence emerge of existential risk. To monitor that risk, governments could form a body modelled on CERN, a particle-physics laboratory, that could also study AI safety and ethics\u2014areas where companies lack incentives to invest as much as society might wish. \nThis powerful technology poses new risks, but also offers extraordinary opportunities. Balancing the two means treading carefully. A measured approach today can provide the foundations on which further rules can be added in future. But the time to start building those foundations is now. \u25a0\nFor subscribers only: to see how we design each week\u2019s cover, sign up to our weekly Cover Story newsletter.", "description": "Rapid progress in AI is arousing fear as well as excitement. How worried should you be?", "ogDescription": "Rapid progress in AI is arousing fear as well as excitement. How worried should you be?", "imageUrl": "https://www.economist.com/media-assets/image/20230422_LDD001.jpg", "imageHeight": 720, "imageWidth": 1280, "datePublished": "2023-04-20T09:41:17Z", "dateModified": "2023-04-20T17:14:19Z", "dateCreated": "2023-04-19T15:17:41Z", "isPrintArticle": true, "printEdition": "2023-04-22T00:00:00Z", "copyrightYear": 2023, "dateline": "", "inLanguage": "en", "interactive": false, "scripts": [], "css": []}, "sectionArticles": [{"id": "/content/tllsauohlcqsej2f7lld8rutcb40qe50", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 200.06, "__typename": "Content"}, "__typename": "Content"}], "headline": "Ukraine\u2019s coming counter-offensive may shape its future\u2014and Europe\u2019s", "url": {"canonical": "https://www.economist.com/leaders/2023/04/20/ukraines-coming-counter-offensive-may-shape-its-future-and-europes", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/2vipu3l1c8g7u5v3tg124qhj2smlhe6j", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 200.07, "__typename": "Content"}, "__typename": "Content"}], "headline": "Bolivia\u2019s crisis shows the limits of left-wing populism", "url": {"canonical": "https://www.economist.com/leaders/2023/04/20/bolivias-crisis-shows-the-limits-of-left-wing-populism", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/aap14j8gd5hhnncgg26ruom74a5ef0rb", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 200.1, "__typename": "Content"}, "__typename": "Content"}], "headline": "Why the world should welcome competition from Chinese carmakers", "url": {"canonical": "https://www.economist.com/leaders/2023/04/20/why-the-world-should-welcome-competition-from-chinese-carmakers", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/4ct5r2etoctqg4eqe86ateetnpo4nje3", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 200.05, "__typename": "Content"}, "__typename": "Content"}], "headline": "How to worry wisely about artificial intelligence", "url": {"canonical": "https://www.economist.com/leaders/2023/04/20/how-to-worry-wisely-about-artificial-intelligence", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/ld3j5vq3je2190ctq11btf3paahu4f7g", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 200.09, "__typename": "Content"}, "__typename": "Content"}], "headline": "Why America will soon see a wave of bank mergers", "url": {"canonical": "https://www.economist.com/leaders/2023/04/20/why-america-will-soon-see-a-wave-of-bank-mergers", "__typename": "URL"}, "__typename": "Content"}, {"id": "/content/ub19a8ro8ljrg4tlkepavkrjoogc255k", "publication": [{"id": "/content/oq02trftu9mc92lsd5itom87ufgqp15q", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/4scc4vva7odlr6m76otihrom34b2lu80", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/fqi3r1ipeqmp3sn7rnquv7getubmh3v2", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/2q9c1n7p82uugfpdffl189mrkemql2f7", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/oebc7jcc10bin6t3e44svq590regi5bv", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/qk3633vlq3r42orc70a3haatrqk3f45s", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}, {"id": "/content/588urv90n1p5co985p8os2oho9o1gj26", "context": {"position": 200.08, "__typename": "Content"}, "__typename": "Content"}], "headline": "In Sudan and beyond, the trend towards global peace has been reversed", "url": {"canonical": "https://www.economist.com/leaders/2023/04/19/in-sudan-and-beyond-the-trend-towards-global-peace-has-been-reversed", "__typename": "URL"}, "__typename": "Content"}]}