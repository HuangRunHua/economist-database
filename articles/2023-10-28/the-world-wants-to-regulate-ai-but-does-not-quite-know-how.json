{"coverImageURL": "https://www.economist.com/media-assets/image/20231028_WBD003.jpg", "coverImageWidth": 1280, "coverImageHeight": 720, "coverImageDescription": "", "title": "The world wants to regulate AI, but does not quite know how", "subtitle": "There is disagreement over what is to be policed, how and by whom", "hashTag": "Business", "authorName": "The Economist", "publishDate": "2023-10-24T19:12:21Z", "contents": [{"role": "body", "text": "The venue will be picturesque: a 19th-century pile north of London that during the second world war was home to Alan Turing, his code-breaking crew and the first programmable digital computer. The attendees will be an elite bunch of 100 world leaders and tech executives. And the question they will strive to answer is epochal: how to ensure that artificial intelligence neither becomes a tool of unchecked malfeasance nor turns against humanity."}, {"role": "body", "text": "The \u201cAI Safety Summit\u201d, which the British government is hosting on November 1st and 2nd at Bletchley Park, appears destined for the history books. And it may indeed one day be seen as the first time global power-brokers sat down to discuss seriously what to do about a technology that may change the world. As Jonathan Black, one of the organisers, observed, in contrast to other big policy debates, such as climate change, \u201cthere is a lot of good will\u201d but \u201cwe still don\u2019t know what the right answer is.\u201d"}, {"role": "body", "text": "Efforts to rein in AI abound. Negotiations in Brussels entered a pivotal stage on October 25th as officials grappled to finalise the European Union\u2019s ambitious AI act by the end of the year. In the days leading up to Britain\u2019s summit or shortly thereafter, the White House is expected to issue an executive order on AI. The G7 club of rich democracies will this autumn start drafting a code of conduct for AI firms. China, for its part, on October 18th unveiled a \u201cGlobal AI Governance Initiative\u201d."}, {"role": "body", "text": "The momentum stems from an unusual political economy. Incentives to act, and act together, are strong. For starters, AI is truly a global technology. Large language models (LLMs), which power eerily humanlike services such as ChatGPT, travel easily. Some can be run on a laptop. It is of little use to tighten the screws on AI in some countries if they remain loose in others. Voters may be in favour. More than half of Americans \u201care more concerned than excited\u201d about the use of AI, according to polling by the Pew Research Centre."}, {"role": "second", "text": "The Beijing effect"}, {"role": "body", "text": "Regulatory rivalry is adding more urgency. Europe\u2019s AI act is intended in part to cement the bloc\u2019s role as the setter of global digital standards. The White House would love to forestall such a \u201cBrussels effect\u201d. Neither the EU nor America wants to be outdone by China, which has already adopted several AI laws. They were cross with the British government for inviting China to the summit\u2014never mind that without it, any regulatory regime would not be truly global. (China may actually show up, even if its interest is less to protect humanity than the Communist Party.)"}, {"role": "body", "text": "Another driver of AI-rulemaking diplomacy is even more surprising: the model-makers themselves. In the past the technology industry mostly opposed regulation. Now giants such as Alphabet and Microsoft, and AI darlings like Anthropic and OpenAI, which created ChatGPT, lobby for it. Companies fret that unbridled competition will push them to act recklessly by releasing models that could easily be abused or start developing minds of their own. That would really land them in hot water."}, {"role": "body", "text": "The will to act is there, in other words. What is not there is \u201canything approaching consensus as to what the problems are that we need to govern, let alone how it is that we ought to govern them\u201d, says Henry Farrell of Johns Hopkins University. Three debates stand out. What should the world worry about? What should any rules target? And how should they be enforced?"}, {"role": "body", "text": "Start with the goals of regulation. These are hard to set because AI is evolving rapidly. Hardly a day passes without a startup coming up with something new. Even the developers of LLMs cannot say for sure what capabilities these will exhibit. This makes it crucial to have tests that can gauge how risky they might be\u2014something that is still more art than science. Without such \u201cevals\u201d (short for evaluations), it will be hard to check whether a model is complying with any rules."}, {"role": "body", "text": "Tech companies may back regulation, but want it to be narrow and target only extreme risks. At a Senate hearing in Washington in July, Dario Amodei, Anthropic\u2019s chief executive, warned that AI models will in a few years be able to provide all the information needed to build bioweapons, enabling \u201cmany more actors to carry out large scale biological attacks\u201d. Similar dire forecasts are being made about cyber-weapons. Earlier this month Gary Gensler, chairman of America\u2019s Securities and Exchange Commission, predicted that an AI-engineered financial crisis was \u201cnearly unavoidable\u201d without swift intervention."}, {"role": "body", "text": "Others argue that these speculative risks distract from other threats, such as undermining the democratic process. At an earlier Senate hearing Gary Marcus, a noted AI sceptic, opened his testimony with a snippet of breaking news written by GPT-4, OpenAI\u2019s top model. It convincingly alleged that parts of Congress were \u201csecretly manipulated by extraterrestrial entities\u201d. \u201cWe should all be deeply worried,\u201d Mr Marcus argued, \u201cabout systems that can fluently confabulate.\u201d "}, {"role": "body", "text": "The debate over what exactly to regulate will be no easier to resolve. Tech firms mostly suggest limiting scrutiny to the most powerful \u201cfrontier\u201d models. Microsoft, among others, has called for a licensing regime requiring firms to register models that exceed certain performance thresholds. Other proposals include controlling the sale of powerful chips used to train LLMs and mandating that cloud-computing firms inform authorities when customers train frontier models."}, {"role": "body", "text": "Most firms also agree it is models\u2019 applications, rather than the models themselves, that ought to be regulated. Office software? Light touch. Health-care AI? Stringent rules. Facial recognition in public spaces? Probably a no-go. The advantage of such use-based regulation is that existing laws would mostly suffice. The AI developers warn that broader and more intrusive rules would slow down innovation. "}, {"role": "body", "text": "Until last year America, Britain and the EU seemed to agree on this risk-based approach. The breathtaking rise of LLMs since the launch of ChatGPT a year ago is giving them second thoughts. The EU is now wondering whether the models themselves need to be overseen, after all. The European Parliament wants model-makers to test LLMs for potential impact on everything from human health to human rights. It insists on getting information about the data on which the models are trained. Canada has a harder-edged \u201cArtificial Intelligence and Data Act\u201d in its parliamentary works. Brazil is discussing something similar. In America, President Joe Biden\u2019s forthcoming executive order is also expected to include some tougher rules. Even Britain may revisit its hands-off approach."}, {"role": "body", "text": "These harder regulations would be a change from non-binding codes of conduct, which have hitherto been the preferred approach. Last summer the White House negotiated a set of \u201cvoluntary commitments\u201d, which 15 model-makers have now signed. The firms agreed to have their models internally and externally tested before release and to share information about how they manage AI risks. "}, {"role": "body", "text": "Then there is the question of who should do the regulating. America and Britain think existing government agencies can do most of the job. The EU wants to create a new regulatory body. Internationally, a few tech executives now call for the creation of something akin to the Intergovernmental Panel on Climate Change (IPCC), which the UN tasks with keeping abreast of research into global warming and with developing ways to gauge its impact."}, {"role": "body", "text": "Given all these open questions, it comes as no surprise that the organisers of the London summit do not sound that ambitious. It should mainly be thought of as \u201ca conversation\u201d, said Mr Black. Still, the not-so-secret hope is that it will yield a few tangible results, in particular on day two when only 20 or so of the most important corporate and world leaders remain in the room. They could yet endorse the White House\u2019s voluntary commitments and recommend the creation of an IPCC for AI or even globalising Britain\u2019s existing \u201cFrontier AI Taskforce\u201d."}, {"role": "body", "text": "Such an outcome would count as a success for Britain\u2019s government. It would also speed up the more official efforts at global AI governance, such as the G7\u2019s code of conduct. As such, it would be a useful first step. It will not be the last. \u25a0"}, {"role": "body", "text": "To stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter."}, {"role": "body", "text": "Read more of our articles on artificial intelligence"}], "id": 51}