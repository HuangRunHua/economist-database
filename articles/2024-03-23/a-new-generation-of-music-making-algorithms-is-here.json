{"coverImageURL": "https://www.economist.com/media-assets/image/20240323_STD001.jpg", "coverImageWidth": 1280, "coverImageHeight": 720, "coverImageDescription": "", "title": "A new generation of music-making algorithms is here", "subtitle": "Their most useful application may lie in helping human composers", "hashTag": "Science & technology", "authorName": "The Economist", "publishDate": "2024-03-21T14:15:17Z", "contents": [{"role": "body", "text": "IN THE dystopia of George Orwell\u2019s novel \u201c1984\u201d, Big Brother numbs the masses with the help of a \u201cversificator\u201d, a machine designed to automatically generate the lyrics to popular tunes, thereby ridding society of human creativity. Today, numerous artificial-intelligence (AI) models churn out, some free of charge, the music itself. Unsurprisingly, many fear a world flooded with generic and emotionally barren tunes, with human musicians edged out in the process. Yet there are brighter signs, too, that AI may well drive a boom in musical creativity."}, {"role": "body", "text": "AI music-making is nothing new. The first, so-called \u201crules-based\u201d, models date to the 1950s. These were built by painstakingly translating principles of music theory into algorithmic instructions and probability tables to determine note and chord progressions. The outputs were musically sound but creatively limited. Ed Newton-Rex, an industry veteran who designed one such model for Jukedeck, a London firm he founded in 2012, describes that approach as good for the day but irrelevant now."}, {"role": "body", "text": "The clearest demonstration that times have changed came in August 2023. That is when Meta, a social-media giant, released the source code for AudioCraft, a suite of large \u201cgenerative\u201d music models built using machine learning. AI outfits worldwide promptly set about using Meta\u2019s software to train new music generators, many with additional code folded in. One AudioCraft model, MusicGen, analysed patterns in some 400,000 recordings with a collective duration of almost 28 months to come up with 3.3bn \u201cparameters\u201d, or variables, that enables the algorithm to generate patterns of sounds in response to prompts. The space this creates for genuinely new AI compositions is unprecedented."}, {"role": "body", "text": "Such models are also getting easier to use. In September Stability AI, a firm based in London at which Mr Newton-Rex worked until recently, released a model, Stable Audio, trained on some 800,000 tracks. Users guide it by entering text and audio clips. This makes it easy to upload, say, a guitar solo and have it recomposed in jazzy piano, perhaps with a vinyl playback feel. Audio prompts are a big deal for two reasons, says Oliver Bown of Australia\u2019s University of New South Wales. First, even skilled musicians struggle to put music into words. Second, because most musical training data are only cursorily tagged, even a large model may not understand a request for, say, a four-bar bridge in ragtime progression (the style familiar from Scott Joplin\u2019s \u201cThe Entertainer\u201d)."}, {"role": "body", "text": "The potential, clearly, is vast. But many in the industry remain sceptical. One widespread sentiment is that AI will never produce true music. That\u2019s because, as a musician friend recently told Yossef Adi, an engineer at Meta\u2019s AI lab in Tel Aviv, \u201cno one broke its heart\u201d. That may be true, but some AI firms reckon that they have found a way to retain and reproduce the \u201cunique musical fingerprint\u201d of their musician users, as LifeScore, a company founded near London, puts it. LifeScore\u2019s AI limits itself to recomposing the elements of a user\u2019s original recordings in ways that maintain the music\u2019s feel, rather than turning them into something radically new."}, {"role": "body", "text": "It takes about a day to plug into LifeScore\u2019s model the dozens of individually recorded vocal and instrumental microphone tracks, or stems, that go into producing an original song. Once that\u2019s done, however, the software, developed at a cost of some $10m, can rework each stem into a new tempo, key or genre within a couple of seconds. The song\u2019s artists, present during the process, choose which remixes to keep. Manually remixing a hit track has traditionally taken one or more highly paid specialists weeks."}, {"role": "body", "text": "LifeScore, says Tom Gruber, a co-founder, is \u201cliterally swamped with requests\u201d from clients including Sony Music, Universal Music Group and Warner Music Group. An original release is typically turned into anywhere from a handful to a dozen remixes. But one client aims to release a dizzying 6,000 or so AI versions of an original track, each targeting a different market. Artists including Pink Floyd\u2019s David Gilmour and Tom Gaebel, a German pop singer, use LifeScore\u2019s AI to power websites that allow fans to generate, with a few clicks, new remixes adapted to personal tastes."}, {"role": "second", "text": "The beat of a different drum"}, {"role": "body", "text": "If this seems like dizzying progress, it\u2019s worth noting that AI\u2019s impact on music is still in its early days. Legal uncertainties over the use of copyrighted recordings to train models have slowed development. Outfits that have coughed up for licensing fees note that this can get expensive. To save on that cost, MusicGen\u2019s training set mostly sidestepped hits, says Dr Adi. Though output is pretty good, he adds, the model is not yet \u201cartistic enough\u201d to generate narratively complete songs. Harmonic misalignments are common. OpenAI, a San Francisco firm, for its part, says its MuseNet model struggles to pull off \u201codd pairings\u201d, such as a Chopin style that incorporates bass and drums."}, {"role": "body", "text": "In time, bigger training sets of better music will largely overcome such shortcomings, developers reckon. A Stability AI spokesperson says that while Stable Audio\u2019s top duration for coherently structured music\u2014\u201cintro, development and outro\u201d\u2014is now about 90 seconds, upgrades will produce longer pieces with \u201cfull musicality\u201d. But judging music AI by its ability to crank out polished tracks mostly misses the point. The technology\u2019s greatest promise, for now at least, lies elsewhere."}, {"role": "body", "text": "Part of it is the empowerment of amateurs. AI handles technical tasks beyond many people\u2019s capabilities and means. As a result, AI is drawing legions of newbies into music-making. This is a boon for experimentation by what Simon Cross, head of products at Native Instruments, a firm based in Berlin, calls \u201cbedroom producers\u201d."}, {"role": "body", "text": "Consider RX, a Native Instruments AI \u201cassistant\u201d that corrects errors in things like pitch and timing. For the latter, software time-shifts notes by cutting out or inserting slivers of sound with matching timbre, a process called \u201cdynamic time-warping\u201d. The company\u2019s AI also determines what mixing and mastering processes were performed on a song of a user\u2019s choosing. It then replicates, or at least approximates, the same expensive processing on the user\u2019s own creations. Boomy, an online \u201cmusic automation\u201d platform for what Alex Mitchell, its ceo, describes as \u201clow-friction\u201d song production with text prompts, has more than 2m users. The company, based in Berkeley, California, uploads users\u2019 (vetted) creations to streaming services and collects a cut of revenues."}, {"role": "body", "text": "AI serves professionals, too. The soundtracks to \u201cBarbie\u201d and \u201cOppenheimer\u201d were cleaned up in post-production with RX, for example. Another application area is \u201cstyle transfer\u201d, in which models transform music recorded with one instrument into sounds that seem to come from a different one, often with a twist or two requested by the user. Style transfers are also used for voice. A model developed by a startup in London called Voice-Swap slices up sounds sung by (remunerated) professional singers and rearranges the slivers into lyrics written by the service\u2019s users, who pay licensing fees for the rights to sell the resulting tracks. And AI tools already exist to recreate singers\u2019 voices in other languages. Vocaloid, a voice-synthesising tool from Yamaha, a Japanese instrument manufacturer, is one of many that can use a translation sung by a native speaker as a template for an AI to imitate as it rearranges, modifies and stitches together tiny snippets of the original singer\u2019s voice."}, {"role": "body", "text": "Accomplished musicians now widely tap MusicGen and its competitors as sources of \u201cinfinite inspirations\u201d, the better to alight upon promising composition ideas, says Meta\u2019s Dr Adi. Whether such inspiration pays off will, ultimately, be up to the listener to decide. \u25a0"}, {"role": "body", "text": "Curious about the world? To enjoy our mind-expanding science coverage, sign up to Simply Science, our weekly subscriber-only newsletter."}], "id": 62}