{"coverImageURL": "https://www.economist.com/media-assets/image/20230415_IRD001.jpg", "coverImageWidth": 1280, "coverImageHeight": 720, "coverImageDescription": "", "title": "Letters to the editor", "subtitle": "A selection of correspondence", "hashTag": "Letters", "authorName": "The Economist", "publishDate": "2023-05-11T13:35:06Z", "contents": [{"role": "body", "text": "Letters are welcome via e-mail to letters@economist.com"}, {"role": "second", "text": "Subtle support for Ukraine"}, {"role": "body", "text": "\u201cHow to survive a superpower split\u201d (April 15th) highlighted how the 25 non-aligned \u201ctransactional\u201d countries you listed are using diplomacy, trade and investment to play more diverse global roles. Yet the behaviour of the T25 also has some commonality with other parts of the world. You say that they have \u201csat on the fence\u201d on the Ukraine war. It is true that this group has not sanctioned Russia or supplied military equipment to Ukraine. They have no record of ever imposing sanctions except when mandated by the UN. However, 19 out of the T25 supported the UN General Assembly resolution of February 23rd 2023 calling on Russia to withdraw from Ukraine. The other six abstained. None voted with Russia. A month later the G20 drafted a statement in similar terms and the Indian foreign minister said that all but Russia and China were ready to support it. And no G20 or T25 country, including China and India, has recognised the Russian annexation of Crimea. "}, {"role": "body", "text": "Paul HareBoston University"}, {"role": "image", "imageURL": "https://www.economist.com/media-assets/image/20230429_USD000.jpg", "imageWidth": 1280.0, "imageHeight": 720.0, "imageDescription": ""}, {"role": "second", "text": "America and Israel"}, {"role": "body", "text": "America\u2019s partnership with Israel has not always run smoothly (Lexington, April 29th). Although Harry Truman did recognise the new state of Israel his administration slapped an arms embargo on it, even prosecuting Americans who broke the ban. Only communist Czechoslovakia was willing to arm Israel. Dwight Eisenhower openly sided with Egypt during the Suez crisis and threatened Israel with UN sanctions. Eventually an ineffective UN peacekeeping force and worthless promises allowed Israel to save face, but the lack of security led directly to another war a decade later. It was John Kennedy who sent American weapons to Israel for the first time. Support for Israel is broad and bipartisan, but it has not always been so."}, {"role": "body", "text": "Charles HallNew York"}, {"role": "image", "imageURL": "https://www.economist.com/media-assets/image/20230506_LDD002.jpg", "imageWidth": 1280.0, "imageHeight": 720.0, "imageDescription": ""}, {"role": "second", "text": "The Turkish election"}, {"role": "body", "text": "The Economist did not surprise us in its misguided attempt to tell Turkish voters how they should act (\u201cThe most important election this year\u201d, May 6th). T\u00fcrkiye is a democratic country that has a long-standing tradition of holding free and fair elections, where governments have been elected into office by popular vote. Trying to belittle and smear the democratic choice of the people, and attempting to lecture them from afar on what they \u201cshould\u201d do comes off as presumptuous to say the least."}, {"role": "body", "text": "Although your coverage dismissed the contributions T\u00fcrkiye made to global and regional security and welfare for decades, my country will continue to be a provider of peace in a volatile region. We deem this a responsibility to our history and people."}, {"role": "body", "text": "Osman Koray Erta\u015fTurkish ambassadorLondon"}, {"role": "image", "imageURL": "https://www.economist.com/media-assets/image/20230422_LDD001.jpg", "imageWidth": 1280.0, "imageHeight": 720.0, "imageDescription": ""}, {"role": "second", "text": "Don\u2019t regulate AI now"}, {"role": "body", "text": "Your recent coverage on artificial intelligence was inspired, though your argument for regulating AI boiled down to comparing the technology to cars, planes and medicines (\u201cHow to worry wisely about AI\u201d, April 22nd). This is not a good analogy. Regulating AI is more like regulating physics or mechanical engineering, in other words it makes no sense. Specific applications of AI can be and are regulated, such as in self-driving cars. Many regulations assume human biases that are not present in AI (see chapter 21 of Daniel Kahneman\u2019s \u201cThinking Fast and Slow\u201d). The question is then whether these products should be regulated differently when AI is involved. "}, {"role": "body", "text": "If AI gives rise to entirely new products then they should be regulated as such, if needed, but it would be quite premature to try to do so now. Regulating a nascent industry like AI opens the door to all sorts of noxious political and special-group interference. Imagine if the same had happened to the internet. The EU\u2019s \u201cregulate first, ask questions later\u201d approach is damaging to both consumers and the European tech industry. Kudos to Britain and America for being more sensible so far."}, {"role": "body", "text": "Pedro DomingosProfessor emeritus, computer science and engineeringUniversity of WashingtonSeattle"}, {"role": "body", "text": "In order to endanger civilisation a super-powered AI would merely need access to money and unsavoury information. It would readily find both of these on the internet. The existence of cryptocurrencies makes the first part easier. A capable AI with access to funds would be able to avail itself of a vast array of professional services used by the very wealthy to hide their identities. It would thereby be able to conduct legitimate business dealings, and also be able to procure illegitimate services (such as contract killings) on the dark net."}, {"role": "body", "text": "The problem is that an AI doesn\u2019t necessarily even need to be \u201csentient\u201d to do these things. A complex enough \u201cmodel\u201d that uses the internet as training data may inadvertently \u201cteach\u201d an AI not only that crime pays, but also how to do it. Now imagine what such a system could accomplish under the direction of a malicious human operator. If we can\u2019t stop a 21-year-old from leaking top-secret documents, how would we stop a disgruntled employee from repurposing a powerful AI for crime?"}, {"role": "body", "text": "In China the unpredictability of complex AI models causes the government to worry that such a model might also \u201clearn\u201d that democracy results in better outcomes for people, and how to circumvent authoritarian controls."}, {"role": "body", "text": "Michael FranzProfessor of computer scienceUniversity of California, Irvine"}, {"role": "body", "text": "We share your assessment that Britain\u2019s \u201clight-touch\u201d approach to regulating AI is unlikely to establish the necessary guardrails to make it safe and reliable. In our comprehensive survey of national AI policies and practices, the Artificial Intelligence and Democratic Values index, we found that countries favour greater regulation as they develop a deeper understanding of the uses of AI. This is true not only in the EU and China, but also in America, where Joe Biden has recently stated that companies should not release commercial AI products that are not safe. The White House has called for an AI bill of rights, and federal agencies, including the Federal Trade Commission, have issued a joint declaration on enforcement efforts against discrimination and bias in automated systems. Chuck Schumer, the leader of the Senate, has made AI a legislative priority."}, {"role": "body", "text": "As for the principles-based approach you propose, one possibility is the Universal Guidelines for Artificial Intelligence, a foundational framework for AI policy that outlines rights and responsibilities for the development and deployment of AI systems to maximise the benefits and minimise the risks. "}, {"role": "body", "text": "Merve HickokPresident"}, {"role": "body", "text": "Lorraine KisselburghChair"}, {"role": "body", "text": "Marc RotenbergExecutive directorCentre for AI and Digital PolicyWashington, DC"}, {"role": "body", "text": "Although I was pleased to see so much attention dedicated to the enormous potential of AI, it was disappointing to see Britain\u2019s plan for regulating this technology described as \u201clight touch\u201d. This seems to imply that we are avoiding regulation, and that is not the case."}, {"role": "body", "text": "Our AI regulation white paper sets out an adaptable, proportionate regulatory framework, underpinned by principles that our expert regulators would apply to ensure good, responsible use of this technology. We are actively seeking feedback as we develop these plans. We want to get the right balance between mitigating risk and driving innovation."}, {"role": "body", "text": "AI presents colossal opportunities, from medical breakthroughs to powering the technology behind self-driving cars. This is an industry that already employs over 50,000 people and contributed \u00a33.7bn ($4.7bn) to the British economy last year. The government has specifically earmarked AI as one of the five critical technologies that could drive job creation and economic growth and transform people\u2019s quality of life. "}, {"role": "body", "text": "We have to seize this moment, and we have already started by committing \u00a3100m to creating a Foundation Model Task Force, to unlock opportunities for safe and responsible innovation in future AI tech. "}, {"role": "body", "text": "VISCOUNT CAMROSEMinister for AI and intellectual propertyDepartment for Science, Innovation and TechnologyLondon"}, {"role": "image", "imageURL": "https://www.economist.com/media-assets/image/20230422_STD002.jpg", "imageWidth": 1280.0, "imageHeight": 720.0, "imageDescription": ""}, {"role": "body", "text": "You did a fine job outlining the risks and general ethical concerns associated with large language models (\u201cHow generative models could go wrong\u201d, April 22nd). However, the new wave of generative AI is unique for its potential to enable manipulative influence at scale. The problem with that goes much deeper than easing the spread of misleading or false information. Effective influence is a prized goal, for nefarious and benevolent actors alike, such as governments, corporations, or alleged Nigerian princes in dire straits. "}, {"role": "body", "text": "To influence effectively you need to understand what makes people tick and then tailor your approach. What makes generative AI unique is that it removes the bottleneck of having to create the tailored influence yourself. Highly persuasive messages, images, and videos are available at the click of a button. Such persuasive influence easily degenerates into manipulation when it does not contribute to understanding and successful inquiry. "}, {"role": "body", "text": "It is a welcome sign that institutions like the EU ponder plans to explicitly outlaw manipulation by AI. For that regulation to have a bite, however, more attention must be paid to the question of how to avoid manipulation and design for influence that aids understanding. Otherwise, I worry that before a \u201cstochastic parrot\u201d can obtain superhuman intelligence, it will do much harm by amplifying (wittingly or unwittingly) manipulation at scale. Although we are not being turned into paper clips just yet, we must ask both quotidian and quintessentially humane questions of how generative AI can contribute to good, meaningful influence. "}, {"role": "body", "text": "Dr Michael KlenkAssistant professor of philosophy and ethics of technologyDelft University of TechnologyDelft, Netherlands"}, {"role": "body", "text": "If an AI can be \u201cpoisoned\u201d to not recognise an apple, or act in a nefarious way, then surely it can also be poisoned into preventing an extinction event with the injection of the right data. Google is ahead of the game in this respect, releasing its killer-robots.txt Easter egg a decade before the rest of us started worrying about AI."}, {"role": "body", "text": "Nick Wills-JohnsonVictoria Park, Australia"}, {"role": "image", "imageURL": "https://www.economist.com/media-assets/image/20230415_WBD002.jpg", "imageWidth": 1280.0, "imageHeight": 720.0, "imageDescription": ""}, {"role": "second", "text": "Mirror image"}, {"role": "body", "text": "Bartleby missed the most important rule of videoconferencing (April 15th). If you wear glasses, you should not surf other web sites during the meeting. The reflection from the monitor in your spectacles will inform the entire group that your attention is elsewhere."}, {"role": "body", "text": "Charles RosenblattCleveland"}], "id": 9}