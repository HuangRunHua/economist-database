{"coverImageURL": "https://www.economist.com/media-assets/image/20230527_WBD000.jpg", "coverImageWidth": 1280, "coverImageHeight": 720, "coverImageDescription": "", "title": "The tech giants have an interest in AI regulation", "subtitle": "It is a way of holding back open-source proliferation", "hashTag": "Business", "authorName": "The Economist", "publishDate": "2023-05-25T12:59:02Z", "contents": [{"role": "body", "text": "One of the joys of writing about business is that rare moment when you realise conventions are shifting in front of you. It brings a shiver down the spine. Vaingloriously, you start scribbling down every detail of your surroundings, as if you are drafting the opening lines of a bestseller. It happened to your columnist recently in San Francisco, sitting in the pristine offices of Anthropic, a darling of the artificial-intelligence (AI) scene. When Jack Clark, one of Anthropic\u2019s co-founders, drew an analogy between the Baruch Plan, a (failed) effort in 1946 to put the world\u2019s atomic weapons under UN control, and the need for global co-ordination to prevent the proliferation of harmful AI, there was that old familiar tingle. When entrepreneurs compare their creations, even tangentially, to nuclear bombs, it feels like a turning point."}, {"role": "body", "text": "Since ChatGPT burst onto the scene late last year there has been no shortage of angst about the existential risks posed by AI. But this is different. Listen to some of the field\u2019s pioneers and they are less worried about a dystopian future when machines outthink humans, and more about the dangers lurking within the stuff they are making now. ChatGPT is an example of \u201cgenerative\u201d ai, which creates humanlike content based on its analysis of texts, images and sounds on the internet. Sam Altman, CEO of OpenAI, the startup that built it, told a congressional hearing this month that regulatory intervention is critical to manage the risks of the increasingly powerful \u201clarge language models\u201d (LLMs) behind the bots."}, {"role": "body", "text": "In the absence of rules, some of his counterparts in San Francisco say they have already set up back channels with government officials in Washington, DC, to discuss the potential harms discovered while examining their chatbots. These include toxic material, such as racism, and dangerous capabilities, like child-grooming or bomb-making. Mustafa Suleyman, co-founder of Inflection AI (and board member of The Economist\u2019s parent company), plans in coming weeks to offer generous bounties to hackers who can discover vulnerabilities in his firm\u2019s digital talking companion, Pi."}, {"role": "body", "text": "Such caution makes this incipient tech boom look different from the past\u2014at least on the surface. As usual, venture capital is rolling in. But unlike the \u201cmove fast and break things\u201d approach of yesteryear, many of the startup pitches now are first and foremost about safety. The old Silicon Valley adage about regulation\u2014that it is better to ask for forgiveness than permission\u2014has been jettisoned. Startups such as OpenAI, Anthropic and Inflection are so keen to convey the idea that they won\u2019t sacrifice safety just to make money that they have put in place corporate structures that constrain profit-maximisation."}, {"role": "body", "text": "Another way in which this boom looks different is that the startups building their proprietary LLMs aren\u2019t aiming to overturn the existing big-tech hierarchy. In fact they may help consolidate it. That is because their relationships with the tech giants leading in the race for generative AI are symbiotic. OpenAI is joined at the hip to Microsoft, a big investor that uses the former\u2019s technology to improve its software and search products. Alphabet\u2019s Google has a sizeable stake in Anthropic; on May 23rd the startup announced its latest funding round of $450m, which included more investment from the tech giant. Making their business ties even tighter, the young firms rely on big tech\u2019s cloud-computing platforms to train their models on oceans of data, which enable the chatbots to behave like human interlocutors."}, {"role": "body", "text": "Like the startups, Microsoft and Google are keen to show they take safety seriously\u2014even as they battle each other fiercely in the chatbot race. They, too, argue that new rules are needed and that international co-operation on overseeing LLMs is essential. As Alphabet\u2019s CEO, Sundar Pichai, put it, \u201cAI is too important not to regulate, and too important not to regulate well.\u201d "}, {"role": "body", "text": "Such overtures may be perfectly justified by the risks of misinformation, electoral manipulation, terrorism, job disruption and other potential hazards that increasingly powerful AI models may spawn. Yet it is worth bearing in mind that regulation will also bring benefits to the tech giants. That is because it tends to reinforce existing market structures, creating costs that incumbents find easiest to bear, and raising barriers to entry."}, {"role": "body", "text": "This is important. If big tech uses regulation to fortify its position at the commanding heights of generative AI, there is a trade-off. The giants are more likely to deploy the technology to make their existing products better than to replace them altogether. They will seek to protect their core businesses (enterprise software in Microsoft\u2019s case and search in Google\u2019s). Instead of ushering in an era of Schumpeterian creative destruction, it will serve as a reminder that large incumbents currently control the innovation process\u2014what some call \u201ccreative accumulation\u201d. The technology may end up being less revolutionary than it could be. "}, {"role": "second", "text": "LLaMA on the loose "}, {"role": "body", "text": "Such an outcome is not a foregone conclusion. One of the wild cards is open-source AI, which has proliferated since March when LLaMa, the LLM developed by Meta, leaked online. Already the buzz in Silicon Valley is that open-source developers are able to build generative-AI models that are almost as good as the existing proprietary ones, and hundredths of the cost. "}, {"role": "body", "text": "Anthropic\u2019s Mr Clark describes open-source AI as a \u201cvery troubling concept\u201d. Though it is a good way of speeding up innovation, it is also inherently hard to control, whether in the hands of a hostile state or a 17-year-old ransomware-maker. Such concerns will be thrashed out as the world\u2019s regulatory bodies grapple with generative AI. Microsoft and Google\u2014and, by extension, their startup charges\u2014have much deeper pockets than open-source developers to handle whatever the regulators come up with. They also have more at stake in preserving the stability of the information-technology system that has turned them into titans. For once, the desire for safety and for profits may be aligned. \u25a0"}, {"role": "body", "text": "Read more from Schumpeter, our columnist on global business:America\u2019s culture wars threaten its single market (May 18th)Writers on strike beware: Hollywood has changed for ever (May 10th)America needs a jab in its corporate backside (May 3rd)"}, {"role": "body", "text": "Also: If you want to write directly to Schumpeter, email him at schumpeter@economist.com. And here is an explanation of how the Schumpeter column got its name."}], "id": 50}